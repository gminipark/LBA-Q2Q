{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/torch2.0/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/envs/torch2.0/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/torch2.0/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023-present the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, multi_modal_model):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        self.multi_modal_model = multi_modal_model\n",
    "        \n",
    "        self.text_model_name = self.multi_modal_model.config._name_or_path\n",
    "        \n",
    "        self.hidden_size = self.multi_modal_model.config.text_config.hidden_size\n",
    "        #self.classifier = nn.Linear(self.hidden_size, 2, dtype=torch.float16, bias=False)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 512  , dtype=torch.float16),\n",
    "            nn.LayerNorm(512, dtype=torch.float16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 2, dtype=torch.float16),\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "       \n",
    "        if \"t5\" in self.text_model_name:\n",
    "            decoder_input_ids = inputs.pop(\"decoder_input_ids\")\n",
    "            outputs = self.multi_modal_model(pixel_values=pixel_values, input_ids=input_ids, decoder_input_ids=decoder_input_ids, output_hidden_states=True, return_dict=True)\n",
    "        \n",
    "            last_hidden_state = outputs.language_model_outputs.decoder_hidden_states[0]\n",
    "            last_token = last_hidden_state[:,-1]\n",
    "            \n",
    "            logits = self.classifier(last_token)\n",
    "            \n",
    "            outputs = logits\n",
    "        \n",
    "        else:\n",
    "            outputs = self.multi_modal_model(pixel_values=pixel_values, input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
    "            last_hidden_state = outputs.language_model_outputs.hidden_states[0] \n",
    "            \n",
    "            logits = self.classifier(last_hidden_state)\n",
    "            \n",
    "            batch_size, sequence_length = input_ids.shape[:2]\n",
    "            sequence_lengths = (torch.ne(input_ids, self.multi_modal_model.config.text_config.pad_token_id).sum(-1) - 1).to(logits.device)\n",
    "            \n",
    "            pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
    "            \n",
    "            outputs = pooled_logits\n",
    "        \n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b4c98d92954de12c\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-b4c98d92954de12c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d0bc0af9ab4af8b342243942533a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\" : \"./ambiguous_questions_train.csv\", \"test\" : \"./ambiguous_questions_test.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3033d82a1c7f45ffa7732a397f8d1c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22388736 || all params: 12251985408 || trainable%: 0.1827355751287555\n"
     ]
    }
   ],
   "source": [
    "# Let's define the LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model_name_or_path = 'Salesforce/blip2-flan-t5-xxl'\n",
    "cache_dir = \"./\" + model_name_or_path.split('/')[-1]\n",
    "\n",
    "# We load our model and processor using `transformers`\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path,cache_dir=cache_dir)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name_or_path,cache_dir=cache_dir, device_map=\"sequential\",torch_dtype=torch.float16)\n",
    "\n",
    "# Get our peft model and print the number of trainable parameters\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#print(model.get_base_model)\n",
    "\n",
    "model = Model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        image = Image.open(\"./images/\"+str(item['image_id'])+\".jpg\")\n",
    "        encoding = self.processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding[\"text\"] = item[\"question\"]\n",
    "        if item['is_ambiguous']:\n",
    "            encoding['label'] = torch.tensor(1) if item['is_ambiguous'] == \"O\" else torch.tensor(0)\n",
    "        \n",
    "        if \"t5\" in self.processor.tokenizer.name_or_path:\n",
    "            encoding['decoder_input_ids'] = torch.tensor([self.processor.tokenizer.pad_token_id])\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "\n",
    "def collator(batch):\n",
    "    # pad the input_ids and attention_mask\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key == \"text\":\n",
    "            text_inputs = processor.tokenizer(\n",
    "                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
    "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "        else:\n",
    "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
    "     \n",
    "    \n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (multi_modal_model): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Blip2ForConditionalGeneration(\n",
       "        (vision_model): Blip2VisionModel(\n",
       "          (embeddings): Blip2VisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "          )\n",
       "          (encoder): Blip2Encoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-38): 39 x Blip2EncoderLayer(\n",
       "                (self_attn): Blip2Attention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (qkv): Linear(\n",
       "                    in_features=1408, out_features=4224, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1408, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=4224, bias=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Blip2MLP(\n",
       "                  (activation_fn): GELUActivation()\n",
       "                  (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "                  (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (qformer): Blip2QFormerModel(\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (encoder): Blip2QFormerEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (2): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (3): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (4): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (5): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (6): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (7): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (8): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (9): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (10): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (crossattention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (11): Blip2QFormerLayer(\n",
       "                (attention): Blip2QFormerAttention(\n",
       "                  (attention): Blip2QFormerMultiHeadAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): Blip2QFormerSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate_query): Blip2QFormerIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output_query): Blip2QFormerOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (language_projection): Linear(in_features=768, out_features=4096, bias=True)\n",
       "        (language_model): T5ForConditionalGeneration(\n",
       "          (shared): Embedding(32128, 4096)\n",
       "          (encoder): T5Stack(\n",
       "            (embed_tokens): Embedding(32128, 4096)\n",
       "            (block): ModuleList(\n",
       "              (0): T5Block(\n",
       "                (layer): ModuleList(\n",
       "                  (0): T5LayerSelfAttention(\n",
       "                    (SelfAttention): T5Attention(\n",
       "                      (q): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                      (v): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                      (relative_attention_bias): Embedding(32, 64)\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (1): T5LayerFF(\n",
       "                    (DenseReluDense): T5DenseGatedActDense(\n",
       "                      (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                      (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                      (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (act): GELUActivation()\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1-23): 23 x T5Block(\n",
       "                (layer): ModuleList(\n",
       "                  (0): T5LayerSelfAttention(\n",
       "                    (SelfAttention): T5Attention(\n",
       "                      (q): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                      (v): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (1): T5LayerFF(\n",
       "                    (DenseReluDense): T5DenseGatedActDense(\n",
       "                      (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                      (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                      (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (act): GELUActivation()\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (final_layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (decoder): T5Stack(\n",
       "            (embed_tokens): Embedding(32128, 4096)\n",
       "            (block): ModuleList(\n",
       "              (0): T5Block(\n",
       "                (layer): ModuleList(\n",
       "                  (0): T5LayerSelfAttention(\n",
       "                    (SelfAttention): T5Attention(\n",
       "                      (q): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                      (v): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                      (relative_attention_bias): Embedding(32, 64)\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (1): T5LayerCrossAttention(\n",
       "                    (EncDecAttention): T5Attention(\n",
       "                      (q): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                      (v): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (2): T5LayerFF(\n",
       "                    (DenseReluDense): T5DenseGatedActDense(\n",
       "                      (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                      (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                      (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (act): GELUActivation()\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1-23): 23 x T5Block(\n",
       "                (layer): ModuleList(\n",
       "                  (0): T5LayerSelfAttention(\n",
       "                    (SelfAttention): T5Attention(\n",
       "                      (q): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                      (v): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (1): T5LayerCrossAttention(\n",
       "                    (EncDecAttention): T5Attention(\n",
       "                      (q): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                      (v): Linear(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (2): T5LayerFF(\n",
       "                    (DenseReluDense): T5DenseGatedActDense(\n",
       "                      (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                      (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                      (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (act): GELUActivation()\n",
       "                    )\n",
       "                    (layer_norm): T5LayerNorm()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (final_layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lm_head): Linear(in_features=4096, out_features=32128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset = ImageTextClassificationDataset(dataset['train'], processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16, collate_fn=collator)\n",
    "\n",
    "test_dataset = ImageTextClassificationDataset(dataset['test'], processor)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=1, collate_fn=collator)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:01<00:09,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84423828125\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:02<00:04,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:02<00:03,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:03<00:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:03<00:01,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:04<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:13,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:12,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:00<00:12,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:12,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:01<00:12,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:01<00:12,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:01<00:11,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:01<00:10,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:02<00:11,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:02<00:11,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:02<00:10,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:03<00:10,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:03<00:09,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:03<00:09,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:03<00:09,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:04<00:09,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:04<00:09,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:04<00:09,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:04<00:08,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:05<00:08,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:05<00:08,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:05<00:07,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:06<00:07,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:06<00:06,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:06<00:06,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:06<00:06,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:07<00:06,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:07<00:06,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:07<00:05,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:07<00:05,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:08<00:05,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:08<00:04,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:08<00:04,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:08<00:04,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [00:09<00:03,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:09<00:03,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:09<00:03,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:09<00:03,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [00:10<00:03,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [00:10<00:02,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [00:10<00:02,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:11<00:02,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [00:11<00:01,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [00:11<00:01,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [00:11<00:01,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [00:12<00:01,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [00:12<00:00,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [00:12<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [00:12<00:00,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [00:13<00:00,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:03,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:01<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:01<00:02,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:02<00:01,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:02<00:01,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:03<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:12,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:12,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:00<00:13,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:13,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:01<00:12,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:01<00:12,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:01<00:11,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:02<00:11,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:02<00:10,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:02<00:10,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:02<00:10,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:03<00:09,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:03<00:09,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:03<00:09,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:03<00:09,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:04<00:09,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:04<00:08,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:04<00:08,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:04<00:08,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:05<00:08,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:05<00:07,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:05<00:07,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:05<00:07,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:06<00:06,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:06<00:06,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:06<00:06,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:07<00:06,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:07<00:05,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:07<00:05,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:07<00:05,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:08<00:04,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:08<00:04,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:08<00:04,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:08<00:04,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [00:09<00:03,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:09<00:03,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5t5t5t5t5t5t5t\n",
      "5t5t5t5t5t5t5t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     42\u001b[0m epoch_outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(test_dataloader)):\n\u001b[1;32m     44\u001b[0m     \u001b[39m# input_ids = batch.pop(\"input_ids\").to(device)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[39m# pixel_values = batch.pop(\"pixel_values\").to(device, dtype=torch.float16)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[39m#generated_output = model.generate(pixel_values=pixel_values, input_ids=input_ids)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     logits \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     49\u001b[0m     predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     50\u001b[0m     predictions \u001b[39m=\u001b[39m [predictions] \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(predictions) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m \u001b[39melse\u001b[39;00m predictions\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m5t5t5t5t5t5t5t\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m decoder_input_ids \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_modal_model(pixel_values\u001b[39m=\u001b[39;49mpixel_values, input_ids\u001b[39m=\u001b[39;49minput_ids, decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     31\u001b[0m last_hidden_state \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlanguage_model_outputs\u001b[39m.\u001b[39mdecoder_hidden_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m last_token \u001b[39m=\u001b[39m last_hidden_state[:,\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/peft/peft_model.py:287\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    284\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39m    Forward pass of the model.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_base_model()(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/transformers/models/blip_2/modeling_blip_2.py:1772\u001b[0m, in \u001b[0;36mBlip2ForConditionalGeneration.forward\u001b[0;34m(self, pixel_values, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, output_attentions, output_hidden_states, labels, return_dict)\u001b[0m\n\u001b[1;32m   1770\u001b[0m         loss \u001b[39m=\u001b[39m loss_fct(shift_logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtext_config\u001b[39m.\u001b[39mvocab_size), shift_labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   1771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1772\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlanguage_model(\n\u001b[1;32m   1773\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1774\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1775\u001b[0m         decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1776\u001b[0m         decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1777\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1778\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1779\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1780\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1781\u001b[0m     )\n\u001b[1;32m   1782\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss \u001b[39mif\u001b[39;00m return_dict \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1783\u001b[0m     logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits \u001b[39mif\u001b[39;00m return_dict \u001b[39melse\u001b[39;00m outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1679\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[39m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1678\u001b[0m     \u001b[39m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1679\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1680\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1681\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1682\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1683\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1684\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1685\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1686\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1687\u001b[0m     )\n\u001b[1;32m   1688\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1689\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1690\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1691\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1692\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1693\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1086\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1074\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1075\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1086\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1087\u001b[0m         hidden_states,\n\u001b[1;32m   1088\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1089\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1090\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1091\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1092\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1093\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1094\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1095\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1096\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1097\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1098\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:753\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    750\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    752\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    755\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:343\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    342\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 343\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    344\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    345\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:314\u001b[0m, in \u001b[0;36mT5DenseGatedActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    312\u001b[0m hidden_linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwi_1(hidden_states)\n\u001b[1;32m    313\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_gelu \u001b[39m*\u001b[39m hidden_linear\n\u001b[0;32m--> 314\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(hidden_states)\n\u001b[1;32m    316\u001b[0m \u001b[39m# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m# See https://github.com/huggingface/transformers/issues/20287\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    320\u001b[0m     \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwo\u001b[39m.\u001b[39mweight, torch\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m    321\u001b[0m     \u001b[39mand\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwo\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    322\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwo\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mint8\n\u001b[1;32m    323\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/conda/envs/torch2.0/lib/python3.9/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "epoch_loss_list = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "with open(\"./ambiguous_questions_test.csv\", 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    lines = [line for line in reader]\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    epoch_loss = []\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device, dtype=torch.float16)\n",
    "        labels = batch.pop(\"label\").to(device)\n",
    "        outputs = model(batch)\n",
    "        \n",
    "        #print(labels)\n",
    "        #print(outputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        print(loss.item())\n",
    "        #loss = torch.mean(outputs)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #if idx % 10 == 0:\n",
    "        #    generated_output = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "        #    print(processor.batch_decode(generated_output, skip_special_tokens=True))\n",
    "    \n",
    "    print(np.mean(epoch_loss))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_outputs = []\n",
    "        for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "            # input_ids = batch.pop(\"input_ids\").to(device)\n",
    "            # pixel_values = batch.pop(\"pixel_values\").to(device, dtype=torch.float16)\n",
    "            #generated_output = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "            \n",
    "            logits = model(batch)\n",
    "            predictions = torch.argmax(logits, dim=1).item()\n",
    "            predictions = [predictions] if type(predictions) != list else predictions\n",
    "            \n",
    "            epoch_outputs += predictions #processor.batch_decode(generated_output, skip_special_tokens=True)\n",
    "    \n",
    "    with open (\"./test_{}.csv\".format(epoch), 'w') as f:\n",
    "        \n",
    "        writer = csv.writer(f)\n",
    "        for idx, line in enumerate(lines):\n",
    "            if idx == 0:\n",
    "                writer.writerow(line)\n",
    "            else:\n",
    "                line.append(epoch_outputs[idx-1])\n",
    "                writer.writerow(line)\n",
    "                \n",
    "    model.train()            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
