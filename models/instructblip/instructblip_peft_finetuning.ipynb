{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/torch2.0/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/envs/torch2.0/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/torch2.0/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023-present the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 42\n",
    "# Fixed RandomSeed\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c679fc7c4c1f415093a764bc639b6888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query_tokens', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.position_embedding', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.post_layernorm.weight', 'vision_model.post_layernorm.bias', 'qformer.embeddings.word_embeddings.weight', 'qformer.embeddings.position_embeddings.weight', 'qformer.embeddings.layernorm.weight', 'qformer.embeddings.layernorm.bias', 'qformer.encoder.layer.0.attention.attention.query.weight', 'qformer.encoder.layer.0.attention.attention.query.bias', 'qformer.encoder.layer.0.attention.attention.key.weight', 'qformer.encoder.layer.0.attention.attention.key.bias', 'qformer.encoder.layer.0.attention.attention.value.weight', 'qformer.encoder.layer.0.attention.attention.value.bias', 'qformer.encoder.layer.0.attention.output.dense.weight', 'qformer.encoder.layer.0.attention.output.dense.bias', 'qformer.encoder.layer.0.attention.output.LayerNorm.weight', 'qformer.encoder.layer.0.attention.output.LayerNorm.bias', 'qformer.encoder.layer.0.crossattention.attention.query.weight', 'qformer.encoder.layer.0.crossattention.attention.query.bias', 'qformer.encoder.layer.0.crossattention.attention.key.weight', 'qformer.encoder.layer.0.crossattention.attention.key.bias', 'qformer.encoder.layer.0.crossattention.attention.value.weight', 'qformer.encoder.layer.0.crossattention.attention.value.bias', 'qformer.encoder.layer.0.crossattention.output.dense.weight', 'qformer.encoder.layer.0.crossattention.output.dense.bias', 'qformer.encoder.layer.0.crossattention.output.LayerNorm.weight', 'qformer.encoder.layer.0.crossattention.output.LayerNorm.bias', 'qformer.encoder.layer.0.intermediate.dense.weight', 'qformer.encoder.layer.0.intermediate.dense.bias', 'qformer.encoder.layer.0.output.dense.weight', 'qformer.encoder.layer.0.output.dense.bias', 'qformer.encoder.layer.0.output.LayerNorm.weight', 'qformer.encoder.layer.0.output.LayerNorm.bias', 'qformer.encoder.layer.0.intermediate_query.dense.weight', 'qformer.encoder.layer.0.intermediate_query.dense.bias', 'qformer.encoder.layer.0.output_query.dense.weight', 'qformer.encoder.layer.0.output_query.dense.bias', 'qformer.encoder.layer.0.output_query.LayerNorm.weight', 'qformer.encoder.layer.0.output_query.LayerNorm.bias', 'qformer.encoder.layer.1.attention.attention.query.weight', 'qformer.encoder.layer.1.attention.attention.query.bias', 'qformer.encoder.layer.1.attention.attention.key.weight', 'qformer.encoder.layer.1.attention.attention.key.bias', 'qformer.encoder.layer.1.attention.attention.value.weight', 'qformer.encoder.layer.1.attention.attention.value.bias', 'qformer.encoder.layer.1.attention.output.dense.weight', 'qformer.encoder.layer.1.attention.output.dense.bias', 'qformer.encoder.layer.1.attention.output.LayerNorm.weight', 'qformer.encoder.layer.1.attention.output.LayerNorm.bias', 'qformer.encoder.layer.1.intermediate.dense.weight', 'qformer.encoder.layer.1.intermediate.dense.bias', 'qformer.encoder.layer.1.output.dense.weight', 'qformer.encoder.layer.1.output.dense.bias', 'qformer.encoder.layer.1.output.LayerNorm.weight', 'qformer.encoder.layer.1.output.LayerNorm.bias', 'qformer.encoder.layer.1.intermediate_query.dense.weight', 'qformer.encoder.layer.1.intermediate_query.dense.bias', 'qformer.encoder.layer.1.output_query.dense.weight', 'qformer.encoder.layer.1.output_query.dense.bias', 'qformer.encoder.layer.1.output_query.LayerNorm.weight', 'qformer.encoder.layer.1.output_query.LayerNorm.bias', 'qformer.encoder.layer.2.attention.attention.query.weight', 'qformer.encoder.layer.2.attention.attention.query.bias', 'qformer.encoder.layer.2.attention.attention.key.weight', 'qformer.encoder.layer.2.attention.attention.key.bias', 'qformer.encoder.layer.2.attention.attention.value.weight', 'qformer.encoder.layer.2.attention.attention.value.bias', 'qformer.encoder.layer.2.attention.output.dense.weight', 'qformer.encoder.layer.2.attention.output.dense.bias', 'qformer.encoder.layer.2.attention.output.LayerNorm.weight', 'qformer.encoder.layer.2.attention.output.LayerNorm.bias', 'qformer.encoder.layer.2.crossattention.attention.query.weight', 'qformer.encoder.layer.2.crossattention.attention.query.bias', 'qformer.encoder.layer.2.crossattention.attention.key.weight', 'qformer.encoder.layer.2.crossattention.attention.key.bias', 'qformer.encoder.layer.2.crossattention.attention.value.weight', 'qformer.encoder.layer.2.crossattention.attention.value.bias', 'qformer.encoder.layer.2.crossattention.output.dense.weight', 'qformer.encoder.layer.2.crossattention.output.dense.bias', 'qformer.encoder.layer.2.crossattention.output.LayerNorm.weight', 'qformer.encoder.layer.2.crossattention.output.LayerNorm.bias', 'qformer.encoder.layer.2.intermediate.dense.weight', 'qformer.encoder.layer.2.intermediate.dense.bias', 'qformer.encoder.layer.2.output.dense.weight', 'qformer.encoder.layer.2.output.dense.bias', 'qformer.encoder.layer.2.output.LayerNorm.weight', 'qformer.encoder.layer.2.output.LayerNorm.bias', 'qformer.encoder.layer.2.intermediate_query.dense.weight', 'qformer.encoder.layer.2.intermediate_query.dense.bias', 'qformer.encoder.layer.2.output_query.dense.weight', 'qformer.encoder.layer.2.output_query.dense.bias', 'qformer.encoder.layer.2.output_query.LayerNorm.weight', 'qformer.encoder.layer.2.output_query.LayerNorm.bias', 'qformer.encoder.layer.3.attention.attention.query.weight', 'qformer.encoder.layer.3.attention.attention.query.bias', 'qformer.encoder.layer.3.attention.attention.key.weight', 'qformer.encoder.layer.3.attention.attention.key.bias', 'qformer.encoder.layer.3.attention.attention.value.weight', 'qformer.encoder.layer.3.attention.attention.value.bias', 'qformer.encoder.layer.3.attention.output.dense.weight', 'qformer.encoder.layer.3.attention.output.dense.bias', 'qformer.encoder.layer.3.attention.output.LayerNorm.weight', 'qformer.encoder.layer.3.attention.output.LayerNorm.bias', 'qformer.encoder.layer.3.intermediate.dense.weight', 'qformer.encoder.layer.3.intermediate.dense.bias', 'qformer.encoder.layer.3.output.dense.weight', 'qformer.encoder.layer.3.output.dense.bias', 'qformer.encoder.layer.3.output.LayerNorm.weight', 'qformer.encoder.layer.3.output.LayerNorm.bias', 'qformer.encoder.layer.3.intermediate_query.dense.weight', 'qformer.encoder.layer.3.intermediate_query.dense.bias', 'qformer.encoder.layer.3.output_query.dense.weight', 'qformer.encoder.layer.3.output_query.dense.bias', 'qformer.encoder.layer.3.output_query.LayerNorm.weight', 'qformer.encoder.layer.3.output_query.LayerNorm.bias', 'qformer.encoder.layer.4.attention.attention.query.weight', 'qformer.encoder.layer.4.attention.attention.query.bias', 'qformer.encoder.layer.4.attention.attention.key.weight', 'qformer.encoder.layer.4.attention.attention.key.bias', 'qformer.encoder.layer.4.attention.attention.value.weight', 'qformer.encoder.layer.4.attention.attention.value.bias', 'qformer.encoder.layer.4.attention.output.dense.weight', 'qformer.encoder.layer.4.attention.output.dense.bias', 'qformer.encoder.layer.4.attention.output.LayerNorm.weight', 'qformer.encoder.layer.4.attention.output.LayerNorm.bias', 'qformer.encoder.layer.4.crossattention.attention.query.weight', 'qformer.encoder.layer.4.crossattention.attention.query.bias', 'qformer.encoder.layer.4.crossattention.attention.key.weight', 'qformer.encoder.layer.4.crossattention.attention.key.bias', 'qformer.encoder.layer.4.crossattention.attention.value.weight', 'qformer.encoder.layer.4.crossattention.attention.value.bias', 'qformer.encoder.layer.4.crossattention.output.dense.weight', 'qformer.encoder.layer.4.crossattention.output.dense.bias', 'qformer.encoder.layer.4.crossattention.output.LayerNorm.weight', 'qformer.encoder.layer.4.crossattention.output.LayerNorm.bias', 'qformer.encoder.layer.4.intermediate.dense.weight', 'qformer.encoder.layer.4.intermediate.dense.bias', 'qformer.encoder.layer.4.output.dense.weight', 'qformer.encoder.layer.4.output.dense.bias', 'qformer.encoder.layer.4.output.LayerNorm.weight', 'qformer.encoder.layer.4.output.LayerNorm.bias', 'qformer.encoder.layer.4.intermediate_query.dense.weight', 'qformer.encoder.layer.4.intermediate_query.dense.bias', 'qformer.encoder.layer.4.output_query.dense.weight', 'qformer.encoder.layer.4.output_query.dense.bias', 'qformer.encoder.layer.4.output_query.LayerNorm.weight', 'qformer.encoder.layer.4.output_query.LayerNorm.bias', 'qformer.encoder.layer.5.attention.attention.query.weight', 'qformer.encoder.layer.5.attention.attention.query.bias', 'qformer.encoder.layer.5.attention.attention.key.weight', 'qformer.encoder.layer.5.attention.attention.key.bias', 'qformer.encoder.layer.5.attention.attention.value.weight', 'qformer.encoder.layer.5.attention.attention.value.bias', 'qformer.encoder.layer.5.attention.output.dense.weight', 'qformer.encoder.layer.5.attention.output.dense.bias', 'qformer.encoder.layer.5.attention.output.LayerNorm.weight', 'qformer.encoder.layer.5.attention.output.LayerNorm.bias', 'qformer.encoder.layer.5.intermediate.dense.weight', 'qformer.encoder.layer.5.intermediate.dense.bias', 'qformer.encoder.layer.5.output.dense.weight', 'qformer.encoder.layer.5.output.dense.bias', 'qformer.encoder.layer.5.output.LayerNorm.weight', 'qformer.encoder.layer.5.output.LayerNorm.bias', 'qformer.encoder.layer.5.intermediate_query.dense.weight', 'qformer.encoder.layer.5.intermediate_query.dense.bias', 'qformer.encoder.layer.5.output_query.dense.weight', 'qformer.encoder.layer.5.output_query.dense.bias', 'qformer.encoder.layer.5.output_query.LayerNorm.weight', 'qformer.encoder.layer.5.output_query.LayerNorm.bias', 'qformer.encoder.layer.6.attention.attention.query.weight', 'qformer.encoder.layer.6.attention.attention.query.bias', 'qformer.encoder.layer.6.attention.attention.key.weight', 'qformer.encoder.layer.6.attention.attention.key.bias', 'qformer.encoder.layer.6.attention.attention.value.weight', 'qformer.encoder.layer.6.attention.attention.value.bias', 'qformer.encoder.layer.6.attention.output.dense.weight', 'qformer.encoder.layer.6.attention.output.dense.bias', 'qformer.encoder.layer.6.attention.output.LayerNorm.weight', 'qformer.encoder.layer.6.attention.output.LayerNorm.bias', 'qformer.encoder.layer.6.crossattention.attention.query.weight', 'qformer.encoder.layer.6.crossattention.attention.query.bias', 'qformer.encoder.layer.6.crossattention.attention.key.weight', 'qformer.encoder.layer.6.crossattention.attention.key.bias', 'qformer.encoder.layer.6.crossattention.attention.value.weight', 'qformer.encoder.layer.6.crossattention.attention.value.bias', 'qformer.encoder.layer.6.crossattention.output.dense.weight', 'qformer.encoder.layer.6.crossattention.output.dense.bias', 'qformer.encoder.layer.6.crossattention.output.LayerNorm.weight', 'qformer.encoder.layer.6.crossattention.output.LayerNorm.bias', 'qformer.encoder.layer.6.intermediate.dense.weight', 'qformer.encoder.layer.6.intermediate.dense.bias', 'qformer.encoder.layer.6.output.dense.weight', 'qformer.encoder.layer.6.output.dense.bias', 'qformer.encoder.layer.6.output.LayerNorm.weight', 'qformer.encoder.layer.6.output.LayerNorm.bias', 'qformer.encoder.layer.6.intermediate_query.dense.weight', 'qformer.encoder.layer.6.intermediate_query.dense.bias', 'qformer.encoder.layer.6.output_query.dense.weight', 'qformer.encoder.layer.6.output_query.dense.bias', 'qformer.encoder.layer.6.output_query.LayerNorm.weight', 'qformer.encoder.layer.6.output_query.LayerNorm.bias', 'qformer.encoder.layer.7.attention.attention.query.weight', 'qformer.encoder.layer.7.attention.attention.query.bias', 'qformer.encoder.layer.7.attention.attention.key.weight', 'qformer.encoder.layer.7.attention.attention.key.bias', 'qformer.encoder.layer.7.attention.attention.value.weight', 'qformer.encoder.layer.7.attention.attention.value.bias', 'qformer.encoder.layer.7.attention.output.dense.weight', 'qformer.encoder.layer.7.attention.output.dense.bias', 'qformer.encoder.layer.7.attention.output.LayerNorm.weight', 'qformer.encoder.layer.7.attention.output.LayerNorm.bias', 'qformer.encoder.layer.7.intermediate.dense.weight', 'qformer.encoder.layer.7.intermediate.dense.bias', 'qformer.encoder.layer.7.output.dense.weight', 'qformer.encoder.layer.7.output.dense.bias', 'qformer.encoder.layer.7.output.LayerNorm.weight', 'qformer.encoder.layer.7.output.LayerNorm.bias', 'qformer.encoder.layer.7.intermediate_query.dense.weight', 'qformer.encoder.layer.7.intermediate_query.dense.bias', 'qformer.encoder.layer.7.output_query.dense.weight', 'qformer.encoder.layer.7.output_query.dense.bias', 'qformer.encoder.layer.7.output_query.LayerNorm.weight', 'qformer.encoder.layer.7.output_query.LayerNorm.bias', 'qformer.encoder.layer.8.attention.attention.query.weight', 'qformer.encoder.layer.8.attention.attention.query.bias', 'qformer.encoder.layer.8.attention.attention.key.weight', 'qformer.encoder.layer.8.attention.attention.key.bias', 'qformer.encoder.layer.8.attention.attention.value.weight', 'qformer.encoder.layer.8.attention.attention.value.bias', 'qformer.encoder.layer.8.attention.output.dense.weight', 'qformer.encoder.layer.8.attention.output.dense.bias', 'qformer.encoder.layer.8.attention.output.LayerNorm.weight', 'qformer.encoder.layer.8.attention.output.LayerNorm.bias', 'qformer.encoder.layer.8.crossattention.attention.query.weight', 'qformer.encoder.layer.8.crossattention.attention.query.bias', 'qformer.encoder.layer.8.crossattention.attention.key.weight', 'qformer.encoder.layer.8.crossattention.attention.key.bias', 'qformer.encoder.layer.8.crossattention.attention.value.weight', 'qformer.encoder.layer.8.crossattention.attention.value.bias', 'qformer.encoder.layer.8.crossattention.output.dense.weight', 'qformer.encoder.layer.8.crossattention.output.dense.bias', 'qformer.encoder.layer.8.crossattention.output.LayerNorm.weight', 'qformer.encoder.layer.8.crossattention.output.LayerNorm.bias', 'qformer.encoder.layer.8.intermediate.dense.weight', 'qformer.encoder.layer.8.intermediate.dense.bias', 'qformer.encoder.layer.8.output.dense.weight', 'qformer.encoder.layer.8.output.dense.bias', 'qformer.encoder.layer.8.output.LayerNorm.weight', 'qformer.encoder.layer.8.output.LayerNorm.bias', 'qformer.encoder.layer.8.intermediate_query.dense.weight', 'qformer.encoder.layer.8.intermediate_query.dense.bias', 'qformer.encoder.layer.8.output_query.dense.weight', 'qformer.encoder.layer.8.output_query.dense.bias', 'qformer.encoder.layer.8.output_query.LayerNorm.weight', 'qformer.encoder.layer.8.output_query.LayerNorm.bias', 'qformer.encoder.layer.9.attention.attention.query.weight', 'qformer.encoder.layer.9.attention.attention.query.bias', 'qformer.encoder.layer.9.attention.attention.key.weight', 'qformer.encoder.layer.9.attention.attention.key.bias', 'qformer.encoder.layer.9.attention.attention.value.weight', 'qformer.encoder.layer.9.attention.attention.value.bias', 'qformer.encoder.layer.9.attention.output.dense.weight', 'qformer.encoder.layer.9.attention.output.dense.bias', 'qformer.encoder.layer.9.attention.output.LayerNorm.weight', 'qformer.encoder.layer.9.attention.output.LayerNorm.bias', 'qformer.encoder.layer.9.intermediate.dense.weight', 'qformer.encoder.layer.9.intermediate.dense.bias', 'qformer.encoder.layer.9.output.dense.weight', 'qformer.encoder.layer.9.output.dense.bias', 'qformer.encoder.layer.9.output.LayerNorm.weight', 'qformer.encoder.layer.9.output.LayerNorm.bias', 'qformer.encoder.layer.9.intermediate_query.dense.weight', 'qformer.encoder.layer.9.intermediate_query.dense.bias', 'qformer.encoder.layer.9.output_query.dense.weight', 'qformer.encoder.layer.9.output_query.dense.bias', 'qformer.encoder.layer.9.output_query.LayerNorm.weight', 'qformer.encoder.layer.9.output_query.LayerNorm.bias', 'qformer.encoder.layer.10.attention.attention.query.weight', 'qformer.encoder.layer.10.attention.attention.query.bias', 'qformer.encoder.layer.10.attention.attention.key.weight', 'qformer.encoder.layer.10.attention.attention.key.bias', 'qformer.encoder.layer.10.attention.attention.value.weight', 'qformer.encoder.layer.10.attention.attention.value.bias', 'qformer.encoder.layer.10.attention.output.dense.weight', 'qformer.encoder.layer.10.attention.output.dense.bias', 'qformer.encoder.layer.10.attention.output.LayerNorm.weight', 'qformer.encoder.layer.10.attention.output.LayerNorm.bias', 'qformer.encoder.layer.10.crossattention.attention.query.weight', 'qformer.encoder.layer.10.crossattention.attention.query.bias', 'qformer.encoder.layer.10.crossattention.attention.key.weight', 'qformer.encoder.layer.10.crossattention.attention.key.bias', 'qformer.encoder.layer.10.crossattention.attention.value.weight', 'qformer.encoder.layer.10.crossattention.attention.value.bias', 'qformer.encoder.layer.10.crossattention.output.dense.weight', 'qformer.encoder.layer.10.crossattention.output.dense.bias', 'qformer.encoder.layer.10.crossattention.output.LayerNorm.weight', 'qformer.encoder.layer.10.crossattention.output.LayerNorm.bias', 'qformer.encoder.layer.10.intermediate.dense.weight', 'qformer.encoder.layer.10.intermediate.dense.bias', 'qformer.encoder.layer.10.output.dense.weight', 'qformer.encoder.layer.10.output.dense.bias', 'qformer.encoder.layer.10.output.LayerNorm.weight', 'qformer.encoder.layer.10.output.LayerNorm.bias', 'qformer.encoder.layer.10.intermediate_query.dense.weight', 'qformer.encoder.layer.10.intermediate_query.dense.bias', 'qformer.encoder.layer.10.output_query.dense.weight', 'qformer.encoder.layer.10.output_query.dense.bias', 'qformer.encoder.layer.10.output_query.LayerNorm.weight', 'qformer.encoder.layer.10.output_query.LayerNorm.bias', 'qformer.encoder.layer.11.attention.attention.query.weight', 'qformer.encoder.layer.11.attention.attention.query.bias', 'qformer.encoder.layer.11.attention.attention.key.weight', 'qformer.encoder.layer.11.attention.attention.key.bias', 'qformer.encoder.layer.11.attention.attention.value.weight', 'qformer.encoder.layer.11.attention.attention.value.bias', 'qformer.encoder.layer.11.attention.output.dense.weight', 'qformer.encoder.layer.11.attention.output.dense.bias', 'qformer.encoder.layer.11.attention.output.LayerNorm.weight', 'qformer.encoder.layer.11.attention.output.LayerNorm.bias', 'qformer.encoder.layer.11.intermediate.dense.weight', 'qformer.encoder.layer.11.intermediate.dense.bias', 'qformer.encoder.layer.11.output.dense.weight', 'qformer.encoder.layer.11.output.dense.bias', 'qformer.encoder.layer.11.output.LayerNorm.weight', 'qformer.encoder.layer.11.output.LayerNorm.bias', 'qformer.encoder.layer.11.intermediate_query.dense.weight', 'qformer.encoder.layer.11.intermediate_query.dense.bias', 'qformer.encoder.layer.11.output_query.dense.weight', 'qformer.encoder.layer.11.output_query.dense.bias', 'qformer.encoder.layer.11.output_query.LayerNorm.weight', 'qformer.encoder.layer.11.output_query.LayerNorm.bias', 'language_projection.weight', 'language_projection.bias', 'language_model.shared.weight', 'language_model.encoder.block.0.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.0.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.0.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.0.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'language_model.encoder.block.0.layer.0.layer_norm.weight', 'language_model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.0.layer.1.layer_norm.weight', 'language_model.encoder.block.1.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.1.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.1.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.1.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.1.layer.0.layer_norm.weight', 'language_model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.1.layer.1.layer_norm.weight', 'language_model.encoder.block.2.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.2.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.2.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.2.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.2.layer.0.layer_norm.weight', 'language_model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.2.layer.1.layer_norm.weight', 'language_model.encoder.block.3.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.3.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.3.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.3.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.3.layer.0.layer_norm.weight', 'language_model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.3.layer.1.layer_norm.weight', 'language_model.encoder.block.4.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.4.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.4.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.4.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.4.layer.0.layer_norm.weight', 'language_model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.4.layer.1.layer_norm.weight', 'language_model.encoder.block.5.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.5.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.5.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.5.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.5.layer.0.layer_norm.weight', 'language_model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.5.layer.1.layer_norm.weight', 'language_model.encoder.block.6.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.6.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.6.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.6.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.6.layer.0.layer_norm.weight', 'language_model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.6.layer.1.layer_norm.weight', 'language_model.encoder.block.7.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.7.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.7.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.7.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.7.layer.0.layer_norm.weight', 'language_model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.7.layer.1.layer_norm.weight', 'language_model.encoder.block.8.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.8.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.8.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.8.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.8.layer.0.layer_norm.weight', 'language_model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.8.layer.1.layer_norm.weight', 'language_model.encoder.block.9.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.9.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.9.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.9.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.9.layer.0.layer_norm.weight', 'language_model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.9.layer.1.layer_norm.weight', 'language_model.encoder.block.10.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.10.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.10.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.10.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.10.layer.0.layer_norm.weight', 'language_model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.10.layer.1.layer_norm.weight', 'language_model.encoder.block.11.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.11.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.11.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.11.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.11.layer.0.layer_norm.weight', 'language_model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.11.layer.1.layer_norm.weight', 'language_model.encoder.block.12.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.12.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.12.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.12.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.12.layer.0.layer_norm.weight', 'language_model.encoder.block.12.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.12.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.12.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.12.layer.1.layer_norm.weight', 'language_model.encoder.block.13.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.13.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.13.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.13.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.13.layer.0.layer_norm.weight', 'language_model.encoder.block.13.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.13.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.13.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.13.layer.1.layer_norm.weight', 'language_model.encoder.block.14.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.14.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.14.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.14.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.14.layer.0.layer_norm.weight', 'language_model.encoder.block.14.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.14.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.14.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.14.layer.1.layer_norm.weight', 'language_model.encoder.block.15.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.15.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.15.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.15.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.15.layer.0.layer_norm.weight', 'language_model.encoder.block.15.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.15.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.15.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.15.layer.1.layer_norm.weight', 'language_model.encoder.block.16.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.16.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.16.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.16.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.16.layer.0.layer_norm.weight', 'language_model.encoder.block.16.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.16.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.16.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.16.layer.1.layer_norm.weight', 'language_model.encoder.block.17.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.17.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.17.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.17.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.17.layer.0.layer_norm.weight', 'language_model.encoder.block.17.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.17.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.17.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.17.layer.1.layer_norm.weight', 'language_model.encoder.block.18.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.18.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.18.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.18.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.18.layer.0.layer_norm.weight', 'language_model.encoder.block.18.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.18.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.18.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.18.layer.1.layer_norm.weight', 'language_model.encoder.block.19.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.19.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.19.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.19.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.19.layer.0.layer_norm.weight', 'language_model.encoder.block.19.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.19.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.19.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.19.layer.1.layer_norm.weight', 'language_model.encoder.block.20.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.20.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.20.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.20.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.20.layer.0.layer_norm.weight', 'language_model.encoder.block.20.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.20.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.20.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.20.layer.1.layer_norm.weight', 'language_model.encoder.block.21.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.21.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.21.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.21.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.21.layer.0.layer_norm.weight', 'language_model.encoder.block.21.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.21.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.21.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.21.layer.1.layer_norm.weight', 'language_model.encoder.block.22.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.22.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.22.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.22.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.22.layer.0.layer_norm.weight', 'language_model.encoder.block.22.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.22.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.22.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.22.layer.1.layer_norm.weight', 'language_model.encoder.block.23.layer.0.SelfAttention.q.weight', 'language_model.encoder.block.23.layer.0.SelfAttention.k.weight', 'language_model.encoder.block.23.layer.0.SelfAttention.v.weight', 'language_model.encoder.block.23.layer.0.SelfAttention.o.weight', 'language_model.encoder.block.23.layer.0.layer_norm.weight', 'language_model.encoder.block.23.layer.1.DenseReluDense.wi_0.weight', 'language_model.encoder.block.23.layer.1.DenseReluDense.wi_1.weight', 'language_model.encoder.block.23.layer.1.DenseReluDense.wo.weight', 'language_model.encoder.block.23.layer.1.layer_norm.weight', 'language_model.encoder.final_layer_norm.weight', 'language_model.decoder.block.0.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.0.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.0.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.0.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'language_model.decoder.block.0.layer.0.layer_norm.weight', 'language_model.decoder.block.0.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.0.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.0.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.0.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.0.layer.1.layer_norm.weight', 'language_model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.0.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.0.layer.2.layer_norm.weight', 'language_model.decoder.block.1.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.1.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.1.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.1.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.1.layer.0.layer_norm.weight', 'language_model.decoder.block.1.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.1.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.1.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.1.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.1.layer.1.layer_norm.weight', 'language_model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.1.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.1.layer.2.layer_norm.weight', 'language_model.decoder.block.2.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.2.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.2.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.2.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.2.layer.0.layer_norm.weight', 'language_model.decoder.block.2.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.2.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.2.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.2.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.2.layer.1.layer_norm.weight', 'language_model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.2.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.2.layer.2.layer_norm.weight', 'language_model.decoder.block.3.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.3.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.3.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.3.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.3.layer.0.layer_norm.weight', 'language_model.decoder.block.3.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.3.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.3.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.3.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.3.layer.1.layer_norm.weight', 'language_model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.3.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.3.layer.2.layer_norm.weight', 'language_model.decoder.block.4.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.4.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.4.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.4.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.4.layer.0.layer_norm.weight', 'language_model.decoder.block.4.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.4.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.4.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.4.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.4.layer.1.layer_norm.weight', 'language_model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.4.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.4.layer.2.layer_norm.weight', 'language_model.decoder.block.5.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.5.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.5.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.5.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.5.layer.0.layer_norm.weight', 'language_model.decoder.block.5.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.5.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.5.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.5.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.5.layer.1.layer_norm.weight', 'language_model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.5.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.5.layer.2.layer_norm.weight', 'language_model.decoder.block.6.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.6.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.6.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.6.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.6.layer.0.layer_norm.weight', 'language_model.decoder.block.6.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.6.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.6.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.6.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.6.layer.1.layer_norm.weight', 'language_model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.6.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.6.layer.2.layer_norm.weight', 'language_model.decoder.block.7.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.7.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.7.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.7.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.7.layer.0.layer_norm.weight', 'language_model.decoder.block.7.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.7.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.7.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.7.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.7.layer.1.layer_norm.weight', 'language_model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.7.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.7.layer.2.layer_norm.weight', 'language_model.decoder.block.8.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.8.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.8.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.8.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.8.layer.0.layer_norm.weight', 'language_model.decoder.block.8.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.8.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.8.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.8.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.8.layer.1.layer_norm.weight', 'language_model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.8.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.8.layer.2.layer_norm.weight', 'language_model.decoder.block.9.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.9.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.9.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.9.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.9.layer.0.layer_norm.weight', 'language_model.decoder.block.9.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.9.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.9.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.9.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.9.layer.1.layer_norm.weight', 'language_model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.9.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.9.layer.2.layer_norm.weight', 'language_model.decoder.block.10.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.10.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.10.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.10.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.10.layer.0.layer_norm.weight', 'language_model.decoder.block.10.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.10.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.10.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.10.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.10.layer.1.layer_norm.weight', 'language_model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.10.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.10.layer.2.layer_norm.weight', 'language_model.decoder.block.11.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.11.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.11.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.11.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.11.layer.0.layer_norm.weight', 'language_model.decoder.block.11.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.11.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.11.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.11.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.11.layer.1.layer_norm.weight', 'language_model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.11.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.11.layer.2.layer_norm.weight', 'language_model.decoder.block.12.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.12.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.12.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.12.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.12.layer.0.layer_norm.weight', 'language_model.decoder.block.12.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.12.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.12.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.12.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.12.layer.1.layer_norm.weight', 'language_model.decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.12.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.12.layer.2.layer_norm.weight', 'language_model.decoder.block.13.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.13.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.13.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.13.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.13.layer.0.layer_norm.weight', 'language_model.decoder.block.13.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.13.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.13.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.13.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.13.layer.1.layer_norm.weight', 'language_model.decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.13.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.13.layer.2.layer_norm.weight', 'language_model.decoder.block.14.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.14.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.14.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.14.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.14.layer.0.layer_norm.weight', 'language_model.decoder.block.14.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.14.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.14.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.14.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.14.layer.1.layer_norm.weight', 'language_model.decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.14.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.14.layer.2.layer_norm.weight', 'language_model.decoder.block.15.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.15.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.15.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.15.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.15.layer.0.layer_norm.weight', 'language_model.decoder.block.15.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.15.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.15.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.15.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.15.layer.1.layer_norm.weight', 'language_model.decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.15.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.15.layer.2.layer_norm.weight', 'language_model.decoder.block.16.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.16.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.16.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.16.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.16.layer.0.layer_norm.weight', 'language_model.decoder.block.16.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.16.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.16.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.16.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.16.layer.1.layer_norm.weight', 'language_model.decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.16.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.16.layer.2.layer_norm.weight', 'language_model.decoder.block.17.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.17.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.17.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.17.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.17.layer.0.layer_norm.weight', 'language_model.decoder.block.17.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.17.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.17.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.17.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.17.layer.1.layer_norm.weight', 'language_model.decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.17.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.17.layer.2.layer_norm.weight', 'language_model.decoder.block.18.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.18.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.18.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.18.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.18.layer.0.layer_norm.weight', 'language_model.decoder.block.18.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.18.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.18.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.18.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.18.layer.1.layer_norm.weight', 'language_model.decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.18.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.18.layer.2.layer_norm.weight', 'language_model.decoder.block.19.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.19.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.19.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.19.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.19.layer.0.layer_norm.weight', 'language_model.decoder.block.19.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.19.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.19.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.19.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.19.layer.1.layer_norm.weight', 'language_model.decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.19.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.19.layer.2.layer_norm.weight', 'language_model.decoder.block.20.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.20.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.20.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.20.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.20.layer.0.layer_norm.weight', 'language_model.decoder.block.20.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.20.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.20.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.20.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.20.layer.1.layer_norm.weight', 'language_model.decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.20.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.20.layer.2.layer_norm.weight', 'language_model.decoder.block.21.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.21.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.21.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.21.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.21.layer.0.layer_norm.weight', 'language_model.decoder.block.21.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.21.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.21.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.21.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.21.layer.1.layer_norm.weight', 'language_model.decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.21.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.21.layer.2.layer_norm.weight', 'language_model.decoder.block.22.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.22.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.22.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.22.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.22.layer.0.layer_norm.weight', 'language_model.decoder.block.22.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.22.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.22.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.22.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.22.layer.1.layer_norm.weight', 'language_model.decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.22.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.22.layer.2.layer_norm.weight', 'language_model.decoder.block.23.layer.0.SelfAttention.q.weight', 'language_model.decoder.block.23.layer.0.SelfAttention.k.weight', 'language_model.decoder.block.23.layer.0.SelfAttention.v.weight', 'language_model.decoder.block.23.layer.0.SelfAttention.o.weight', 'language_model.decoder.block.23.layer.0.layer_norm.weight', 'language_model.decoder.block.23.layer.1.EncDecAttention.q.weight', 'language_model.decoder.block.23.layer.1.EncDecAttention.k.weight', 'language_model.decoder.block.23.layer.1.EncDecAttention.v.weight', 'language_model.decoder.block.23.layer.1.EncDecAttention.o.weight', 'language_model.decoder.block.23.layer.1.layer_norm.weight', 'language_model.decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'language_model.decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'language_model.decoder.block.23.layer.2.DenseReluDense.wo.weight', 'language_model.decoder.block.23.layer.2.layer_norm.weight', 'language_model.decoder.final_layer_norm.weight', 'language_model.lm_head.weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InstructBlipForConditionalGeneration(\n",
       "  (vision_model): InstructBlipVisionModel(\n",
       "    (embeddings): InstructBlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InstructBlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-38): 39 x InstructBlipEncoderLayer(\n",
       "          (self_attn): InstructBlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): InstructBlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (qformer): InstructBlipQFormerModel(\n",
       "    (embeddings): InstructBlipQFormerEmbeddings(\n",
       "      (word_embeddings): Embedding(30523, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): InstructBlipQFormerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Linear(in_features=768, out_features=2048, bias=True)\n",
       "  (language_model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 2048)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 2048)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 32)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 2048)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 32)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = 'Salesforce/instructblip-flan-t5-xl'\n",
    "cache_dir = \"./\" + model_name_or_path.split('/')[-1]\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "# We load our model and processor using `transformers`\n",
    "processor = InstructBlipProcessor.from_pretrained(model_name_or_path,cache_dir=cache_dir)\n",
    "model = InstructBlipForConditionalGeneration.from_pretrained(model_name_or_path,cache_dir=cache_dir, torch_dtype=dtype)\n",
    "\n",
    "# Get our peft model and print the number of trainable parameters\n",
    "print([name for name,p in model.named_parameters()])\n",
    "\n",
    "for param in model.vision_model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model = Model(model)\n",
    "\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-86ee3f83d7c3c7b3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-86ee3f83d7c3c7b3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00412e2dcb6143aca216499c99abbdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b45fd4a130e4ddab939bb41d693ad8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946961551b6c4dd5aea4769097b0cebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch2.0/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-86ee3f83d7c3c7b3/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-168447e99049af9a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-168447e99049af9a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab133add1a0442c9fa85efc86b39892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f52d118c8c4587be11e366c6035aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc0865f38c34ced9ea6cdfcdbc25824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-168447e99049af9a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "Dataset({\n",
      "    features: ['q_id', 'image_id', 'ambiguous_question', 'ambiguous_entity', 'intermediate_question', 'intermediate_answer', 'entity_id', 'labels'],\n",
      "    num_rows: 3343\n",
      "})\n",
      "Dataset({\n",
      "    features: ['q_id', 'image_id', 'ambiguous_question', 'ambiguous_entity', 'intermediate_question', 'intermediate_answer', 'entity_id', 'labels'],\n",
      "    num_rows: 367\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch2.0/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"csv\", data_files={\"train\" : \"./train_6400_same_true_aug_v2.csv\"}, split=\"train\")\n",
    "test_dataset = load_dataset(\"csv\", data_files={\"test\" : \"./test_367_same_true_v2.csv\"}, split=\"test\")\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        image = Image.open(\"./images/\"+str(item['image_id'])+\".jpg\")\n",
    "      \n",
    "        # remove batch dimension\n",
    "        encoding = {}\n",
    "        encoding['image']=image\n",
    "        encoding[\"text\"] = \"Given an ambiguous quesiton, an ambigous entity and an intermediate question, your task is to classify whether the intermediate question clarifies the ambigious entity in the ambiguous quesiton.\" # The ambiguous entity means that it appears multiple times in the image and cannot be distinctly identified. A good intermediate question is one that clarify a specific entity among same entities. Additionaly, A bad intermediate question is one that can't determine one entity among the entities through the intermediate quesiton.  If you think the given intermediate question is good, indicate it by answering \\\"Yes\\\". Otherwise, answer \\\"No\\\".There are only two types of answers possible: \\\"Yes\\\" and \\\"No\\\".\"\n",
    "        encoding[\"text\"] = encoding[\"text\"] + \" Ambiguous question: \" + item[\"ambiguous_question\"] + \"Ambiguous entity: \" + item[\"ambiguous_entity\"] + \" Intermediate question: \" + item[\"intermediate_question\"] + \" Short answer:\"\n",
    "        \n",
    "        # encoding[\"text\"] = \"Ambiguous question: \" + item[\"ambiguous_question\"] +\" Ambigous entity: \" + item[\"ambiguous_entity\"] + \" Intermediate question: \" + item[\"intermediate_question\"] # + \" Intermediate answer: \" + item[\"intermediate_answer\"]\n",
    "        # encoding[\"text\"] = encoding['text'] + \" Is the intermediate question effective to clarify the ambiguous entity in the ambiguous question? Classify yes or no. Short answer: \"\n",
    "        \n",
    "        \n",
    "        if 'effectiveness' in item.keys():\n",
    "            encoding['label'] = \"yes\" if item['effectiveness'] == \"O\" else 'no' # torch.tensor(1) if item['effectiveness'] == \"O\" else torch.tensor(0)\n",
    "        elif 'labels' in item.keys():\n",
    "            encoding['label'] =  \"Yes\" if item['labels'] == \"O\" else 'No' # item['labels']\n",
    "        else:\n",
    "            encoding['label'] = encoding['text']\n",
    "\n",
    "        \n",
    "        if \"t5\" in self.processor.tokenizer.name_or_path:\n",
    "            encoding['decoder_input_ids'] = torch.tensor([self.processor.tokenizer.pad_token_id])\n",
    "        \n",
    "        inputs = processor(images=encoding['image'],text=encoding['text'],return_tensors=\"pt\", max_length=128, padding='max_length' ,truncation=True)\n",
    "        encoding.pop('image')\n",
    "        encoding.pop('text')\n",
    "        encoding.update(inputs)\n",
    "        return encoding\n",
    "\n",
    "\n",
    "def collator(batch):\n",
    "    # pad the input_ids and attention_mask\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():       \n",
    "        if key == \"label\":\n",
    "            labels = processor.tokenizer([example['label'] for example in batch], padding='max_length', add_special_tokens=True, return_tensors='pt')\n",
    "            processed_batch['labels'] = labels['input_ids']\n",
    "        else:\n",
    "            processed_batch[key] = torch.stack([example[key].squeeze() for example in batch])\n",
    "     \n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = ImageTextClassificationDataset(train_dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1, collate_fn=collator)\n",
    "\n",
    "test_dataset = ImageTextClassificationDataset(test_dataset, processor)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=4, collate_fn=collator)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "  1%|          | 25/3343 [00:13<30:23,  1.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/LBA-ARVQA/instructblip_peft_train_effectiveness.ipynb 셀 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgemini_container/workspace/LBA-ARVQA/instructblip_peft_train_effectiveness.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_dataloader)):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgemini_container/workspace/LBA-ARVQA/instructblip_peft_train_effectiveness.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgemini_container/workspace/LBA-ARVQA/instructblip_peft_train_effectiveness.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m         batch[key] \u001b[39m=\u001b[39m batch[key]\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgemini_container/workspace/LBA-ARVQA/instructblip_peft_train_effectiveness.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mt5\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m model_name_or_path:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgemini_container/workspace/LBA-ARVQA/instructblip_peft_train_effectiveness.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "epoch_loss_list = []\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# with open(\"./ambiguous_questions_test.csv\", 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     lines = [line for line in reader]\n",
    "\n",
    "def compute_acc(predictions, references):\n",
    "    \n",
    "    total_len = len(predictions)\n",
    "    same_count = 0\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        if prediction == reference:\n",
    "            same_count += 1\n",
    "    \n",
    "    return same_count / total_len\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    epoch_loss = []\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(device)\n",
    "            \n",
    "        if \"t5\" in model_name_or_path:\n",
    "            decoder_input_ids = batch.pop(\"decoder_input_ids\").to(device)\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        else:\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        #print(labels)\n",
    "        #print(outputs)\n",
    "        \n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = outputs.loss\n",
    "        #print(loss.item())\n",
    "        #loss = torch.mean(outputs)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #if idx % 10 == 0:\n",
    "        #    generated_output = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "        #    print(processor.batch_decode(generated_output, skip_special_tokens=True))\n",
    "    \n",
    "    print(np.mean(epoch_loss))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_outputs = []\n",
    "        gold_references = []\n",
    "        # metric = load(\"accuracy\")\n",
    "        for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "            for key in batch.keys():\n",
    "                batch[key] = batch[key].to(device)\n",
    "            \n",
    "            # if \"t5\" in model_name_or_path:\n",
    "            #     decoder_input_ids = batch.pop(\"decoder_input_ids\").to(device)\n",
    "            #     logits = model(pixel_values, input_ids, decoder_input_ids)\n",
    "            # else:\n",
    "            outputs = model.generate(**batch)\n",
    "            predictions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            references = processor.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "            # metric.add_batch(predictions=predictions, references=references)\n",
    "            \n",
    "            epoch_outputs += predictions #processor.batch_decode(generated_output, skip_special_tokens=True)\n",
    "            gold_references += references\n",
    "            \n",
    "        #accuracy = metric.compute()\n",
    "        print(epoch_outputs[:10])\n",
    "        print(gold_references[:10])\n",
    "        print(compute_acc(epoch_outputs , gold_references))\n",
    "        \n",
    "    # with open (\"./test_{}.csv\".format(epoch), 'w') as f:\n",
    "        \n",
    "    #     writer = csv.writer(f)\n",
    "    #     for idx, line in enumerate(lines):\n",
    "    #         if idx == 0:\n",
    "    #             writer.writerow(line)\n",
    "    #         else:\n",
    "    #             line.append(epoch_outputs[idx-1])\n",
    "    #             writer.writerow(line)\n",
    "                \n",
    "    model.train()            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
