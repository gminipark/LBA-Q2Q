{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023-present the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884316d0c9974db99926360a2d26669b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = 'Salesforce/instructblip-flan-t5-xxl'\n",
    "cache_dir = \"./\" + model_name_or_path.split('/')[-1]\n",
    "\n",
    "# We load our model and processor using `transformers`\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path,cache_dir=cache_dir)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name_or_path,cache_dir=cache_dir,torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InstructBlipForConditionalGeneration(\n",
       "  (vision_model): InstructBlipVisionModel(\n",
       "    (embeddings): InstructBlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InstructBlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-38): 39 x InstructBlipEncoderLayer(\n",
       "          (self_attn): InstructBlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): InstructBlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (qformer): InstructBlipQFormerModel(\n",
       "    (embeddings): InstructBlipQFormerEmbeddings(\n",
       "      (word_embeddings): Embedding(30523, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): InstructBlipQFormerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): InstructBlipQFormerLayer(\n",
       "          (attention): InstructBlipQFormerAttention(\n",
       "            (attention): InstructBlipQFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): InstructBlipQFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate_query): InstructBlipQFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): InstructBlipQFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Linear(in_features=768, out_features=4096, bias=True)\n",
       "  (language_model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 4096)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 4096)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 64)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 4096)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 64)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): GELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32128, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125854\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "few_shot_examples = {}\n",
    "\n",
    "intermediate_questions =  pd.read_excel(\"intermediate_questions_samples_32.xlsx\", dtype={'qid' : str})\n",
    "\n",
    "with open(\"./ambiguous_questions_rebuilt_ptb.json\") as f:\n",
    "    ambiguous_questions = json.load(f)\n",
    "\n",
    "with open(\"./ambiguous_questions.json\") as f:\n",
    "    original_questions = json.load(f)\n",
    "\n",
    "print(len(ambiguous_questions))\n",
    "for idx in intermediate_questions.index:\n",
    "    intermediate_question_example = intermediate_questions.iloc[idx]\n",
    "    qid = intermediate_question_example['qid']\n",
    "\n",
    "    ambiguous_question_example = ambiguous_questions[qid]\n",
    "    \n",
    "    ambiguous_question = ambiguous_question_example['question']\n",
    "    intermediate_question = intermediate_question_example['intermediate question']\n",
    "    original_question_example = original_questions.pop(qid)\n",
    "    \n",
    "    few_shot_examples[qid] = original_question_example\n",
    "    few_shot_examples[qid]['ambiguous_question'] = ambiguous_question\n",
    "    few_shot_examples[qid]['intermediate_question'] = intermediate_question\n",
    "    few_shot_examples[qid].pop(\"addtional_question\")\n",
    "    \n",
    "    entities_list = set()\n",
    "    for object_name in few_shot_examples[qid]['irrelated_object_names'].values():\n",
    "        if object_name in few_shot_examples[qid]['ambiguous_question']:\n",
    "             entities_list.add(object_name)\n",
    "    few_shot_examples[qid]['question_entities'] = list(entities_list)\n",
    "   \n",
    "np.random.seed(42) \n",
    "test_keys = np.random.choice(list(original_questions.keys()), 32, replace=False)\n",
    "test_examples = {}\n",
    "\n",
    "for qid in test_keys:\n",
    "    original_questions[qid].pop(\"addtional_question\")\n",
    "    test_examples[qid] =  original_questions[qid]\n",
    "    \n",
    "    ambiguous_question_example = ambiguous_questions[qid]\n",
    "    ambiguous_question = ambiguous_question_example['question']\n",
    "    \n",
    "    test_examples[qid][\"ambiguous_question\"] = ambiguous_question\n",
    "    \n",
    "    entities_list = set()\n",
    "    for object_name in test_examples[qid]['irrelated_object_names'].values():\n",
    "        if object_name in test_examples[qid]['ambiguous_question']:\n",
    "             entities_list.add(object_name)\n",
    "    test_examples[qid]['question_entities'] = list(entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main question is Is the bus to the left or to the right of the pedestrians? The ambiguous entities of the main question: pedestrians. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch2.0/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "2it [00:01,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What side of the street are the pedestrians on?\n",
      "Main question is What is the food to the left of the doughnut the bucket is to the right of? The ambiguous entities of the main question: bucket. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:02,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is to the right of the doughnut?\n",
      "Main question is How large is the bear? The ambiguous entities of the main question: bear. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:02,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what kind of bear is this?\n",
      "Main question is Is the bus to the right or to the left of the driver? The ambiguous entities of the main question: bus. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:03,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the bus to the right or to the left of the driver?\n",
      "Main question is Is the blue car to the right or to the left of the van? The ambiguous entities of the main question: van. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:03,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the blue car to the right or to the left of the van?\n",
      "Main question is Is the elephant? The ambiguous entities of the main question: elephant. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:04,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the elephant in water?\n",
      "Main question is Which side is the picture on? The ambiguous entities of the main question: picture. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:04,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picture is on which side of tv?\n",
      "Main question is What device is the girl holding , a phone or a Wii controller? The ambiguous entities of the main question: girl. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:05,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What device is the girl holding, a phone or a Wii controller?\n",
      "Main question is What is the food on the plate called? The ambiguous entities of the main question: plate. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:05,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the food on the plate called?\n",
      "Main question is Is the person to the left of the laptop looking at a television? The ambiguous entities of the main question: laptop. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:06,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the person to the right of the laptop looking at a television?\n",
      "Main question is Is the chair to the left of the remote control that is to the left of the girl? The ambiguous entities of the main question: remote control. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:06,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is to the left of the girl?\n",
      "Main question is What is the person to the left of the man holding? The ambiguous entities of the main question: man. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:07,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the person to the left of the man holding?\n",
      "Main question is Is the house to the right or to the left of the cow? The ambiguous entities of the main question: cow. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:07,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the cow to the left or right of the house?\n",
      "Main question is What color is the zebra? The ambiguous entities of the main question: zebra. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:08,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color are the stripes on the zebra?\n",
      "Main question is What is the elephant? The ambiguous entities of the main question: elephant. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:08,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the elephant doing?\n",
      "Main question is What is the vegetable to the left of the cucumber called? The ambiguous entities of the main question: cucumber. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:09,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the vegetable to the left of the cucumber called?\n",
      "Main question is Is she to the left of a picture? The ambiguous entities of the main question: picture. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:09,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is she to the right of a picture?\n",
      "Main question is What shape does the table have? The ambiguous entities of the main question: table. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:09,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What shape is the table?\n",
      "Main question is What is the item of furniture to the left of the speaker? The ambiguous entities of the main question: speaker. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:10,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the item of furniture to the right of the speaker?\n",
      "Main question is Are the glasses to the right or to the left of the person? The ambiguous entities of the main question: person glasses. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:10,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are glasses on right or left of person?\n",
      "Main question is Which side of the picture is the car on? The ambiguous entities of the main question: car. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:11,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which side of the picture is the car on?\n",
      "Main question is What is the appliance to the right of the woman sitting on top of? The ambiguous entities of the main question: woman. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:11,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the woman sitting on top of?\n",
      "Main question is What is on the building behind the window? The ambiguous entities of the main question: window. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:11,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is on the building behind the window?\n",
      "Main question is Are the people? The ambiguous entities of the main question: people. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:12,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are there people in picture\n",
      "Main question is What color is the shirt? The ambiguous entities of the main question: shirt. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:12,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is the shirt?\n",
      "Main question is What is the player wearing? The ambiguous entities of the main question: player. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:13,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is player wearing?\n",
      "Main question is Is the person in the snow wearing a backpack? The ambiguous entities of the main question: snow. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:13,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the person in the snow wearing a backpack?\n",
      "Main question is What type of device is open , the cell phone or the microphone? The ambiguous entities of the main question: cell phone. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:13,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What type of phone is this?\n",
      "Main question is Is the man to the right of the people sitting on a bench? The ambiguous entities of the main question: bench. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:14,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the man to the left of the people sitting on a bench?\n",
      "Main question is What is the item of furniture that is to the right of the curtains? The ambiguous entities of the main question: curtains. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:14,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is to the left of the curtains?\n",
      "Main question is Is the car? The ambiguous entities of the main question: car. Write a intermediate question to clarify the ambiguous entities of the main question. Question: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:15,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the car in front of bus?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "promt = f\"Instructions : Classify the following into ambigous and definite question. An ambiguous question is unanswerable question considering image. \"\n",
    "text = \"\"\n",
    "\n",
    "predictions = []\n",
    "csv_lines = [[\"AQ\", \"IQ\"]]\n",
    "for idx,(qid, example) in tqdm(enumerate(test_examples.items())):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    question = example[\"ambiguous_question\"]\n",
    "    image_id = example['imageId']\n",
    "    image = Image.open(\"./images/\" + str(image_id) + '.jpg').convert('RGB')\n",
    "    \n",
    "    input_prompt = text + \\\n",
    "    f\"Main question is {example['ambiguous_question']} \" +  \\\n",
    "    \"The ambiguous entities of the main question: \" +  \" \".join(example['question_entities']) + \". \" + \\\n",
    "    \"Write a intermediate question to clarify the ambiguous entities of the main question. \" + \\\n",
    "    f\"Question: \"\n",
    "\n",
    "    # \"the intermediate question is not same with the main question.\" + \\\n",
    "    \n",
    "    \n",
    "    print(input_prompt)\n",
    "    inputs = processor(image, input_prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    out = model.generate(**inputs,\n",
    "                         do_sample=False,\n",
    "                        num_beams=5,\n",
    "                        max_length=256,\n",
    "                        min_length=3,\n",
    "                        top_p=0.9,\n",
    "                        repetition_penalty=1.5,\n",
    "                        length_penalty=1.0,\n",
    "                        temperature=1,)\n",
    "    prediction = processor.decode(out[0], skip_special_tokens=True)\n",
    "    example['intermediate_question'] = prediction\n",
    "    print(prediction)\n",
    "    predictions.append(prediction)\n",
    "    csv_lines.append([question, prediction])\n",
    "    \n",
    "with open (f\"./test_fewshot_{model_name_or_path.split('/')[-1]}.json\" , 'w') as f:\n",
    "     \n",
    "    json.dump(test_examples, f)   \n",
    "    # writer = csv.writer(f)\n",
    "    # for idx, line in enumerate(test_lines):\n",
    "    #     if idx == 0:\n",
    "    #         writer.writerow(line)\n",
    "    #     else:\n",
    "    #         line.append(predictions[idx-1])\n",
    "    #         writer.writerow(line)\n",
    "\n",
    "with open (f\"./test_fewshot_{model_name_or_path.split('/')[-1]}.csv\" , 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(csv_lines)       \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
