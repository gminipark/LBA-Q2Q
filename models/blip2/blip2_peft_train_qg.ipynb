{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023-present the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator, DistributedType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model_name_or_path = 'Salesforce/blip2-flan-t5-xl'\n",
    "cache_dir = \"./\" + model_name_or_path.split('/')[-1]\n",
    "\n",
    "dtype = torch.float16  \n",
    "# We load our model and processor using `transformers`\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path,cache_dir=cache_dir)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name_or_path,cache_dir=cache_dir, torch_dtype=dtype)\n",
    "\n",
    "# Get our peft model and print the number of trainable parameters\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model = Model(model)\n",
    "\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"csv\", data_files={\"train\" : \"./IQG_train.csv\"}, split=\"train\")\n",
    "train_dataset = train_dataset.filter(lambda example: example['label'] == \"O\")\n",
    "valid_dataset = load_dataset(\"csv\", data_files={\"validation\" : \"./IQG_val.csv\"}, split=\"validation\")\n",
    "valid_dataset = valid_dataset.filter(lambda example: example['label'] == \"O\")\n",
    "test_dataset = load_dataset(\"csv\", data_files={\"test\" : \"./IQG_test.csv\"}, split=\"test\")\n",
    "test_dataset = test_dataset.filter(lambda example: example['label'] == \"O\")\n",
    "\n",
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        image = Image.open(\"./ambiguous_images/\"+str(item['image_id'])+\".jpg\")\n",
    "        encoding = self.processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding[\"text\"] = \"Ambiguous question: \" + str(item[\"ambiguous_question\"]) +\" Ambigous entity: \" + str(item[\"ambiguous_entity\"])\n",
    "        encoding[\"text\"] = encoding['text'] + \"Generate a question to clarify the ambiguous entity in the ambiguous question?\"\n",
    "        # if 'effectiveness' in item.keys():\n",
    "        #     encoding['label'] = \"yes\" if item['effectiveness'] == \"O\" else 'no' # torch.tensor(1) if item['effectiveness'] == \"O\" else torch.tensor(0)\n",
    "        # elif 'labels' in item.keys():\n",
    "        #     encoding['label'] = item['labels']# encoding['label'] = \"Yes\" if item['labels'] == \"O\" else 'No'\n",
    "        # else:\n",
    "        encoding['label'] = item['additional_question']\n",
    "\n",
    "        \n",
    "        if \"t5\" in self.processor.tokenizer.name_or_path:\n",
    "            encoding['decoder_input_ids'] = torch.tensor([self.processor.tokenizer.pad_token_id])\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "\n",
    "def collator(batch):\n",
    "    # pad the input_ids and attention_mask\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key == \"text\":\n",
    "            text_inputs = processor.tokenizer(\n",
    "                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
    "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "            \n",
    "        elif key == \"label\":\n",
    "            labels = processor.tokenizer([example['label'] for example in batch], padding=True, add_special_tokens=True, return_tensors='pt')\n",
    "            processed_batch['label'] = labels['input_ids']\n",
    "        else:\n",
    "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
    "     \n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = ImageTextClassificationDataset(train_dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32, collate_fn=collator, num_workers=4)\n",
    "\n",
    "valid_dataset = ImageTextClassificationDataset(valid_dataset, processor)\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=8, collate_fn=collator)\n",
    "\n",
    "test_dataset = ImageTextClassificationDataset(test_dataset, processor)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=8, collate_fn=collator)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "epoch_loss_list = []\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# with open(\"./ambiguous_questions_test.csv\", 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     lines = [line for line in reader]\n",
    "\n",
    "def compute_acc(predictions, references):\n",
    "    \n",
    "    total_len = len(predictions)\n",
    "    same_count = 0\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        if prediction == reference:\n",
    "            same_count += 1\n",
    "    \n",
    "    return same_count / total_len\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    epoch_loss = []\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device, dtype=dtype)\n",
    "        labels = batch.pop(\"label\").to(device)\n",
    "        if \"t5\" in model_name_or_path:\n",
    "            decoder_input_ids = batch.pop(\"decoder_input_ids\").to(device)\n",
    "            outputs = model(pixel_values=pixel_values, input_ids=input_ids, labels=labels)\n",
    "        \n",
    "        else:\n",
    "            outputs = model(pixel_values=pixel_values, input_ids=input_ids, labels=labels)\n",
    "        \n",
    "        #print(labels)\n",
    "        #print(outputs)\n",
    "        \n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = outputs.loss\n",
    "        #print(loss.item())\n",
    "        #loss = torch.mean(outputs)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #if idx % 10 == 0:\n",
    "        #    generated_output = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "        #    print(processor.batch_decode(generated_output, skip_special_tokens=True))\n",
    "    \n",
    "    print(np.mean(epoch_loss))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        epoch_outputs = []\n",
    "        gold_references = []\n",
    "        metric = load(\"bleu\")\n",
    "        for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "            input_ids = batch.pop(\"input_ids\").to(device)\n",
    "            pixel_values = batch.pop(\"pixel_values\").to(device, dtype=dtype)\n",
    "            labels = batch.pop(\"label\").to(device)\n",
    "            # if \"t5\" in model_name_or_path:\n",
    "            #     decoder_input_ids = batch.pop(\"decoder_input_ids\").to(device)\n",
    "            #     logits = model(pixel_values, input_ids, decoder_input_ids)\n",
    "            # else:\n",
    "            outputs = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "            predictions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            references = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "            # metric.add_batch(predictions=predictions, references=references)\n",
    "            \n",
    "            epoch_outputs += predictions #processor.batch_decode(generated_output, skip_special_tokens=True)\n",
    "            gold_references += references\n",
    "            \n",
    "        #accuracy = metric.compute()\n",
    "        #print(epoch_outputs)\n",
    "        #print(gold_references)\n",
    "        gold_references = [[gold_ref] for gold_ref in gold_references]\n",
    "        print(epoch_outputs[:2])\n",
    "        print(gold_references[:2])\n",
    "        print(metric.compute(predictions=epoch_outputs , references=gold_references))\n",
    "    \n",
    "    \n",
    "        with open (\"./test_{}.csv\".format(epoch), 'w') as f:\n",
    "            \n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"generation\", \"gold\"])\n",
    "            for prediction, gold_ref in zip(epoch_outputs, gold_references):\n",
    "                writer.writerow([prediction, gold_ref[-1]])\n",
    "                \n",
    "    model.train()            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trainer_ddp():\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(10):\n",
    "    epoch_loss = []\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device, dtype=dtype)\n",
    "        labels = batch.pop(\"label\").to(device)\n",
    "        if \"t5\" in model_name_or_path:\n",
    "            decoder_input_ids = batch.pop(\"decoder_input_ids\").to(device)\n",
    "            outputs = model(pixel_values=pixel_values, input_ids=input_ids, labels=labels)\n",
    "        \n",
    "        else:\n",
    "            outputs = model(pixel_values=pixel_values, input_ids=input_ids, labels=labels)\n",
    "        \n",
    "        #print(labels)\n",
    "        #print(outputs)\n",
    "        \n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = outputs.loss\n",
    "        #print(loss.item())\n",
    "        #loss = torch.mean(outputs)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #if idx % 10 == 0:\n",
    "        #    generated_output = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "        #    print(processor.batch_decode(generated_output, skip_special_tokens=True))\n",
    "    \n",
    "    print(np.mean(epoch_loss))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_outputs = []\n",
    "        gold_references = []\n",
    "        metric = load(\"bleu\")\n",
    "        for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "            input_ids = batch.pop(\"input_ids\").to(device)\n",
    "            pixel_values = batch.pop(\"pixel_values\").to(device, dtype=dtype)\n",
    "            labels = batch.pop(\"label\").to(device)\n",
    "            # if \"t5\" in model_name_or_path:\n",
    "            #     decoder_input_ids = batch.pop(\"decoder_input_ids\").to(device)\n",
    "            #     logits = model(pixel_values, input_ids, decoder_input_ids)\n",
    "            # else:\n",
    "            outputs = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "            predictions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            references = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "            # metric.add_batch(predictions=predictions, references=references)\n",
    "            \n",
    "            epoch_outputs += predictions #processor.batch_decode(generated_output, skip_special_tokens=True)\n",
    "            gold_references += references\n",
    "            \n",
    "        #accuracy = metric.compute()\n",
    "        #print(epoch_outputs)\n",
    "        #print(gold_references)\n",
    "        gold_references = [[gold_ref] for gold_ref in gold_references]\n",
    "        print(epoch_outputs[:2])\n",
    "        print(gold_references[:2])\n",
    "        print(metric.compute(predictions=epoch_outputs , references=gold_references))\n",
    "        \n",
    "    with open (\"./test_{}.csv\".format(epoch), 'w') as f:\n",
    "        \n",
    "        writer = csv.writer(f)\n",
    "        for idx, line in enumerate(epoch_outputs):\n",
    "            if idx == 0:\n",
    "                writer.writerow(line)\n",
    "            else:\n",
    "                line.append(epoch_outputs[idx-1])\n",
    "                writer.writerow(line)\n",
    "                \n",
    "    model.train()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
