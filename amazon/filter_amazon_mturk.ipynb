{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "is_clean = False\n",
    "target_file = \"batch_results_4000.csv\"\n",
    "\n",
    "clean_csv_file = []\n",
    "same_tag_list = []\n",
    "diff_tag_list = []\n",
    "with open(target_file, 'r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    clean_df = pd.DataFrame()\n",
    "    \n",
    "    if is_clean:\n",
    "        clean_df = df\n",
    "        if 'labels' not in clean_df.keys():\n",
    "            clean_df = clean_df.rename(columns={\"effectiveness\" : \"labels\"})\n",
    "    \n",
    "    else:\n",
    "        for key in df.keys():\n",
    "            if \"Input.\" in key:\n",
    "                new_key = key.split('Input.')[-1]\n",
    "                clean_df[new_key] = df[key]\n",
    "            \n",
    "            if \"Answer\" in key:\n",
    "                clean_df[\"labels\"] = df[key] \n",
    "\n",
    "    for idx in range(len(clean_df)):\n",
    "        line = clean_df.iloc[idx]\n",
    "        \n",
    "        line_dict = line.to_dict()\n",
    "        line_dict['labels'] = 'O' if line_dict['labels']=='[{\"effectiveness\":\"yes\"}]' or line_dict['labels'] == 'O' else 'X'\n",
    "        \n",
    "        found = False\n",
    "        for sample in samples:\n",
    "            if sample['q_id'] == line_dict['q_id'] and sample['intermediate_question'] == line_dict['intermediate_question']:\n",
    "                found=True\n",
    "                if sample['labels'] == line_dict['labels']:\n",
    "                    same_tag_list.append(line_dict)\n",
    "                else:\n",
    "                    diff_tag_list.append(sample)\n",
    "                    diff_tag_list.append(line_dict)\n",
    "        if not found:\n",
    "            samples.append(line_dict)\n",
    "            \n",
    "print(len(clean_df))\n",
    "print(len(same_tag_list))\n",
    "print(len(diff_tag_list) / 2)\n",
    "\n",
    "assert len(same_tag_list) * 2 + len(diff_tag_list) == len(clean_df)\n",
    "\n",
    "same_df = pd.DataFrame(same_tag_list)\n",
    "diff_df = pd.DataFrame(diff_tag_list)\n",
    "\n",
    "file_name = Path(target_file).stem\n",
    "\n",
    "clean_df.sort_values(\"q_id\")\n",
    "same_df.sort_values(\"q_id\")\n",
    "diff_df.sort_values(\"q_id\")\n",
    "\n",
    "clean_df.to_csv(file_name+\"_clean.csv\",index=False)\n",
    "same_df.to_csv(file_name+\"_same.csv\",index=False)\n",
    "diff_df.to_csv(file_name+\"_diff.csv\",index=False)\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    " \n",
    "print(Counter(same_df['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "csv_files = glob(\"*_results_*_same.csv\")\n",
    "\n",
    "split_unit = 200\n",
    "test_nums = 100\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={'train' : csv_files})\n",
    "dataset.shuffle(42)\n",
    "dataset = dataset['train'].train_test_split(test_size=test_nums)\n",
    "\n",
    "test_dict = dataset['test']\n",
    "test_dataset = pd.DataFrame(test_dict)\n",
    "test_dataset.to_csv(f\"test_{test_nums}_same.csv\", index=False)\n",
    "print(Counter(dataset['train']['labels']))\n",
    "print(Counter(test_dict['labels']))\n",
    "\n",
    "stop = False\n",
    "while(not stop):\n",
    "    train_dict = dataset['train'].sort('labels', reverse=True)[:split_unit]\n",
    "    train_dataset = pd.DataFrame(train_dict)\n",
    "    train_dataset.to_csv(f\"train_{split_unit}_same_bal.csv\", index=False)\n",
    "    split_unit = split_unit * 2\n",
    "    \n",
    "    if split_unit > len(dataset['train']) - test_nums:\n",
    "        stop = True\n",
    "    print(Counter(train_dict[\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "same_csv_files = glob(\"*_results_*_same.csv\")\n",
    "diff_csv_files = glob(\"*_results_*_diff.csv\")\n",
    "\n",
    "split_unit = 200\n",
    "test_nums = 100\n",
    "\n",
    "same_dataset = load_dataset(\"csv\", data_files={'train' : same_csv_files})\n",
    "diff_dataset = load_dataset(\"csv\", data_files={'train' : diff_csv_files})\n",
    "\n",
    "diff_dataset = diff_dataset.filter(lambda example: example['labels'] == \"X\")\n",
    "\n",
    "print(diff_dataset['train'])\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = concatenate_datasets([same_dataset['train'], diff_dataset['train']])\n",
    "print(dataset)\n",
    "dataset.shuffle(42)\n",
    "\n",
    "\n",
    "dataset = dataset['train'].train_test_split(test_size=test_nums)\n",
    "\n",
    "test_dict = dataset['test']\n",
    "test_dataset = pd.DataFrame(test_dict)\n",
    "test_dataset.to_csv(f\"test_{test_nums}_same_diff.csv\", index=False)\n",
    "print(Counter(dataset['train']['labels']))\n",
    "print(Counter(test_dict['labels']))\n",
    "\n",
    "stop = False\n",
    "while(not stop):\n",
    "    #train_dict = dataset['train'].sort('labels', reverse=True)[:split_unit]\n",
    "    train_dict = dataset['train'][:split_unit]\n",
    "    train_dataset = pd.DataFrame(train_dict)\n",
    "    train_dataset.to_csv(f\"train_{split_unit}_same_diff.csv\", index=False)\n",
    "    split_unit = split_unit * 2\n",
    "    \n",
    "    if split_unit > len(dataset['train']) - test_nums:\n",
    "        stop = True\n",
    "    print(Counter(train_dict[\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def augment_dataset(samples, augment_nums):\n",
    "    \n",
    "    new_samples = {}\n",
    "    print(len(samples['labels']))\n",
    "    for key in samples.keys():\n",
    "        for i in range(augment_nums):\n",
    "            if i == 0:\n",
    "                new_samples[key] = samples[key].copy()\n",
    "            else:\n",
    "                new_samples[key].extend(samples[key].copy())\n",
    "    \n",
    "    print(len(samples['labels']))\n",
    "    \n",
    "    return new_samples\n",
    "        \n",
    "        \n",
    "csv_files = glob(\"*_results_*_same.csv\")\n",
    "\n",
    "split_unit = 200\n",
    "test_nums = 100\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={'train' : csv_files})\n",
    "dataset.shuffle(42)\n",
    "\n",
    "dataset = dataset['train'].train_test_split(test_size=test_nums,seed=42)\n",
    "\n",
    "test_dict = dataset['test']\n",
    "test_dataset = pd.DataFrame(test_dict)\n",
    "test_dataset.to_csv(f\"test_{test_nums}_same.csv\", index=False)\n",
    "print(Counter(dataset['train']['labels']))\n",
    "print(Counter(test_dict['labels']))\n",
    "\n",
    "stop = False\n",
    "while(not stop):\n",
    "    \n",
    "    true_examples = dataset.filter(lambda example: example[\"labels\"] == \"O\",with_indices=False)['train']\n",
    "    false_examples = dataset.filter(lambda example: example[\"labels\"] == \"X\",with_indices=False)['train']\n",
    "    \n",
    "    if len(false_examples) < split_unit // 2:\n",
    "        \n",
    "        augment_nums = split_unit // 2 // len(false_examples)\n",
    "        \n",
    "        print(augment_nums)\n",
    "        augmented_examples = false_examples.map(augment_dataset ,batched=True ,fn_kwargs={'augment_nums':augment_nums},load_from_cache_file=False,keep_in_memory=False)\n",
    "        print(augmented_examples)\n",
    "        \n",
    "        false_examples=augmented_examples\n",
    "    \n",
    "\n",
    "    true_examples=true_examples.select(range(split_unit // 2))\n",
    "    if len(false_examples) >= (split_unit // 2):\n",
    "        false_examples=false_examples.select(range(split_unit // 2))\n",
    "    \n",
    "    train_dict = concatenate_datasets([true_examples, false_examples])    \n",
    "\n",
    "    train_dataset = pd.DataFrame(train_dict)\n",
    "    train_dataset.to_csv(f\"train_{split_unit}_same_balan.csv\", index=False)\n",
    "    split_unit = split_unit * 2\n",
    "    \n",
    "    if split_unit // 2  > len(dataset['train']):\n",
    "        stop = True\n",
    "    print(Counter(train_dict[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "def make_negative_samples(samples, intermediate_questions):\n",
    "    \n",
    "    total_negative_samples=[]\n",
    "    for idx, image_id in enumerate(samples['image_id']):\n",
    "        intermediate_questions_list = intermediate_questions[str(image_id)][str(samples['entity_id'][idx])]\n",
    "        negative_samples=[]\n",
    "        for intermediate_question in intermediate_questions_list:\n",
    "            if intermediate_question['label'] == \"negative\":\n",
    "                negative_sample = dict()\n",
    "                for key in samples.keys():\n",
    "                    negative_sample[key] = samples[key][idx]\n",
    "                \n",
    "                negative_sample['intermediate_question'] = intermediate_question['question']\n",
    "                negative_sample['intermediate_answer'] = intermediate_question['answer']\n",
    "                negative_sample['labels'] = \"X\"\n",
    "                negative_samples.append(negative_sample)\n",
    "        total_negative_samples.append(negative_samples)\n",
    "\n",
    "    return {\"data\" : total_negative_samples}\n",
    "\n",
    "\n",
    "def make_negative_dataset(positive_dataset, intermediate_questions):\n",
    "    #positive_dataset = DatasetDict({\"positive_samples\" : positive_dataset})\n",
    "    print(positive_dataset)\n",
    "    negative_dataset = positive_dataset.map(make_negative_samples, batched=True, fn_kwargs={\"intermediate_questions\" : intermediate_questions},)\n",
    "\n",
    "    ngative_samples_list = [sample for negative_samples in negative_dataset['data'] for sample in negative_samples]\n",
    "\n",
    "    df = pd.DataFrame(ngative_samples_list)\n",
    "    print(len(df))\n",
    "    unique_df = df.drop_duplicates()\n",
    "\n",
    "    negative_dataset = Dataset.from_pandas(unique_df,preserve_index=False)\n",
    "    \n",
    "    return negative_dataset\n",
    "\n",
    "\n",
    "# csv_files = glob(\"*_results_*_same.csv\")\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files=csv_files)\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# positive_dataset = dataset.filter(lambda example: example[\"labels\"] == \"O\",with_indices=False)\n",
    "\n",
    "# # with open(\"./sceneGraphs/train_sceneGraphs.json\") as f:\n",
    "# #     sceneGraphs = json.load(f)\n",
    "    \n",
    "# with open(\"./intermediate_questions.json\") as f:\n",
    "#     intermediate_questions = json.load(f)\n",
    "    \n",
    "# negative_dataset = make_negative_dataset(positive_dataset['train'], intermediate_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "csv_files = glob(\"*_results_*_same.csv\")\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={'train' : csv_files})\n",
    "dataset.shuffle(42)\n",
    "\n",
    "\n",
    "true_examples = dataset.filter(lambda example: example[\"labels\"] == \"O\",with_indices=False)['train']\n",
    "#print(true_examples)\n",
    "false_examples = dataset.filter(lambda example: example[\"labels\"] == \"X\",with_indices=False)['train']\n",
    "#print(len(false_examples))\n",
    "\n",
    "true_dataset = true_examples.train_test_split(test_size=len(false_examples),seed=42)\n",
    "false_dataset = DatasetDict()\n",
    "false_dataset['test'] = false_examples\n",
    "\n",
    "split_unit = 200\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset_list = [true_dataset, false_dataset]\n",
    "for key in true_dataset.keys():\n",
    "    dataset[key] = concatenate_datasets([target_dataset[key] for target_dataset in dataset_list if key in target_dataset.keys()])\n",
    "\n",
    "print(dataset)\n",
    "test_dict = dataset['test']\n",
    "test_nums = len(test_dict)\n",
    "test_dataset = pd.DataFrame(test_dict)\n",
    "test_dataset.to_csv(f\"test_{test_nums}_same.csv\", index=False)\n",
    "print(Counter(dataset['train']['labels']))\n",
    "print(Counter(test_dict['labels']))\n",
    "\n",
    "with open(\"./intermediate_questions.json\") as f:\n",
    "        intermediate_questions = json.load(f)\n",
    "\n",
    "stop = False\n",
    "while(not stop):\n",
    "\n",
    "    true_examples=dataset['train'].select(range(split_unit // 2))\n",
    "    \n",
    "    \n",
    "#     if len(false_examples) < split_unit // 2:\n",
    "        \n",
    "    augment_nums = split_unit // 2 // len(false_examples)\n",
    "    \n",
    "    print(augment_nums)\n",
    "    \n",
    "    negative_dataset = make_negative_dataset(true_examples, intermediate_questions)\n",
    "    \n",
    "    \n",
    "    false_examples=negative_dataset\n",
    "    \n",
    "\n",
    "    true_examples=true_examples.select(range(split_unit // 2))\n",
    "    if len(false_examples) >= (split_unit // 2):\n",
    "        false_examples=false_examples.select(range(split_unit // 2))\n",
    "    \n",
    "    train_dict = concatenate_datasets([true_examples, false_examples])    \n",
    "\n",
    "    train_dataset = pd.DataFrame(train_dict)\n",
    "    train_dataset.to_csv(f\"train_{split_unit}_same_aug.csv\", index=False)\n",
    "    split_unit = split_unit * 2\n",
    "    \n",
    "    if split_unit // 2  > len(dataset['train']):\n",
    "        stop = True\n",
    "    print(Counter(train_dict[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
