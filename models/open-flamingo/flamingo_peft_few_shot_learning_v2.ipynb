{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023-present the HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125854\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "few_shot_examples = {}\n",
    "\n",
    "intermediate_questions =  pd.read_excel(\"intermediate_questions_samples_32.xlsx\", dtype={'qid' : str})\n",
    "\n",
    "with open(\"./ambiguous_questions_rebuilt_ptb.json\") as f:\n",
    "    ambiguous_questions = json.load(f)\n",
    "\n",
    "with open(\"./ambiguous_questions.json\") as f:\n",
    "    original_questions = json.load(f)\n",
    "\n",
    "print(len(ambiguous_questions))\n",
    "for idx in intermediate_questions.index:\n",
    "    intermediate_question_example = intermediate_questions.iloc[idx]\n",
    "    qid = intermediate_question_example['qid']\n",
    "\n",
    "    ambiguous_question_example = ambiguous_questions[qid]\n",
    "    \n",
    "    ambiguous_question = ambiguous_question_example['question']\n",
    "    intermediate_question = intermediate_question_example['intermediate question']\n",
    "    original_question_example = original_questions.pop(qid)\n",
    "    \n",
    "    few_shot_examples[qid] = original_question_example\n",
    "    few_shot_examples[qid]['ambiguous_question'] = ambiguous_question\n",
    "    few_shot_examples[qid]['intermediate_question'] = intermediate_question\n",
    "    few_shot_examples[qid].pop(\"addtional_question\")\n",
    "    \n",
    "    entities_list = set()\n",
    "    for object_name in few_shot_examples[qid]['irrelated_object_names'].values():\n",
    "        if object_name in few_shot_examples[qid]['ambiguous_question']:\n",
    "             entities_list.add(object_name)\n",
    "    few_shot_examples[qid]['question_entities'] = list(entities_list)\n",
    "   \n",
    "np.random.seed(42) \n",
    "test_keys = np.random.choice(list(original_questions.keys()), 32, replace=False)\n",
    "test_examples = {}\n",
    "\n",
    "for qid in test_keys:\n",
    "    original_questions[qid].pop(\"addtional_question\")\n",
    "    test_examples[qid] =  original_questions[qid]\n",
    "    \n",
    "    ambiguous_question_example = ambiguous_questions[qid]\n",
    "    ambiguous_question = ambiguous_question_example['question']\n",
    "    \n",
    "    test_examples[qid][\"ambiguous_question\"] = ambiguous_question\n",
    "    \n",
    "    entities_list = set()\n",
    "    for object_name in test_examples[qid]['irrelated_object_names'].values():\n",
    "        if object_name in test_examples[qid]['ambiguous_question']:\n",
    "             entities_list.add(object_name)\n",
    "    test_examples[qid]['question_entities'] = list(entities_list)\n",
    "    \n",
    "# dataset = load_dataset(\"csv\", data_files={\"train\" : \"./ambiguous_questions_train.csv\", \"test\" : \"./ambiguous_questions_test.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2714a9d79b247e7b86338d833d72c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1384781840 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-7b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-7b\",\n",
    "    cross_attn_every_n_layers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['vision_encoder.class_embedding', 'vision_encoder.positional_embedding', 'vision_encoder.proj', 'vision_encoder.conv1.weight', 'vision_encoder.ln_pre.weight', 'vision_encoder.ln_pre.bias', 'vision_encoder.transformer.resblocks.0.ln_1.weight', 'vision_encoder.transformer.resblocks.0.ln_1.bias', 'vision_encoder.transformer.resblocks.0.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.0.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.0.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.0.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.0.ln_2.weight', 'vision_encoder.transformer.resblocks.0.ln_2.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_1.weight', 'vision_encoder.transformer.resblocks.1.ln_1.bias', 'vision_encoder.transformer.resblocks.1.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.1.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.1.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.1.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_2.weight', 'vision_encoder.transformer.resblocks.1.ln_2.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_1.weight', 'vision_encoder.transformer.resblocks.2.ln_1.bias', 'vision_encoder.transformer.resblocks.2.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.2.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.2.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.2.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_2.weight', 'vision_encoder.transformer.resblocks.2.ln_2.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_1.weight', 'vision_encoder.transformer.resblocks.3.ln_1.bias', 'vision_encoder.transformer.resblocks.3.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.3.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.3.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.3.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_2.weight', 'vision_encoder.transformer.resblocks.3.ln_2.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_1.weight', 'vision_encoder.transformer.resblocks.4.ln_1.bias', 'vision_encoder.transformer.resblocks.4.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.4.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.4.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.4.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_2.weight', 'vision_encoder.transformer.resblocks.4.ln_2.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_1.weight', 'vision_encoder.transformer.resblocks.5.ln_1.bias', 'vision_encoder.transformer.resblocks.5.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.5.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.5.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.5.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_2.weight', 'vision_encoder.transformer.resblocks.5.ln_2.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_1.weight', 'vision_encoder.transformer.resblocks.6.ln_1.bias', 'vision_encoder.transformer.resblocks.6.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.6.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.6.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.6.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_2.weight', 'vision_encoder.transformer.resblocks.6.ln_2.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_1.weight', 'vision_encoder.transformer.resblocks.7.ln_1.bias', 'vision_encoder.transformer.resblocks.7.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.7.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.7.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.7.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_2.weight', 'vision_encoder.transformer.resblocks.7.ln_2.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_1.weight', 'vision_encoder.transformer.resblocks.8.ln_1.bias', 'vision_encoder.transformer.resblocks.8.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.8.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.8.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.8.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_2.weight', 'vision_encoder.transformer.resblocks.8.ln_2.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_1.weight', 'vision_encoder.transformer.resblocks.9.ln_1.bias', 'vision_encoder.transformer.resblocks.9.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.9.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.9.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.9.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_2.weight', 'vision_encoder.transformer.resblocks.9.ln_2.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_1.weight', 'vision_encoder.transformer.resblocks.10.ln_1.bias', 'vision_encoder.transformer.resblocks.10.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.10.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.10.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.10.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_2.weight', 'vision_encoder.transformer.resblocks.10.ln_2.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_1.weight', 'vision_encoder.transformer.resblocks.11.ln_1.bias', 'vision_encoder.transformer.resblocks.11.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.11.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.11.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.11.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_2.weight', 'vision_encoder.transformer.resblocks.11.ln_2.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_1.weight', 'vision_encoder.transformer.resblocks.12.ln_1.bias', 'vision_encoder.transformer.resblocks.12.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.12.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.12.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.12.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_2.weight', 'vision_encoder.transformer.resblocks.12.ln_2.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_1.weight', 'vision_encoder.transformer.resblocks.13.ln_1.bias', 'vision_encoder.transformer.resblocks.13.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.13.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.13.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.13.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_2.weight', 'vision_encoder.transformer.resblocks.13.ln_2.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_1.weight', 'vision_encoder.transformer.resblocks.14.ln_1.bias', 'vision_encoder.transformer.resblocks.14.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.14.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.14.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.14.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_2.weight', 'vision_encoder.transformer.resblocks.14.ln_2.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_1.weight', 'vision_encoder.transformer.resblocks.15.ln_1.bias', 'vision_encoder.transformer.resblocks.15.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.15.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.15.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.15.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_2.weight', 'vision_encoder.transformer.resblocks.15.ln_2.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_1.weight', 'vision_encoder.transformer.resblocks.16.ln_1.bias', 'vision_encoder.transformer.resblocks.16.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.16.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.16.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.16.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_2.weight', 'vision_encoder.transformer.resblocks.16.ln_2.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_1.weight', 'vision_encoder.transformer.resblocks.17.ln_1.bias', 'vision_encoder.transformer.resblocks.17.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.17.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.17.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.17.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_2.weight', 'vision_encoder.transformer.resblocks.17.ln_2.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_1.weight', 'vision_encoder.transformer.resblocks.18.ln_1.bias', 'vision_encoder.transformer.resblocks.18.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.18.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.18.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.18.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_2.weight', 'vision_encoder.transformer.resblocks.18.ln_2.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_1.weight', 'vision_encoder.transformer.resblocks.19.ln_1.bias', 'vision_encoder.transformer.resblocks.19.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.19.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.19.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.19.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_2.weight', 'vision_encoder.transformer.resblocks.19.ln_2.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_1.weight', 'vision_encoder.transformer.resblocks.20.ln_1.bias', 'vision_encoder.transformer.resblocks.20.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.20.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.20.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.20.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_2.weight', 'vision_encoder.transformer.resblocks.20.ln_2.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_1.weight', 'vision_encoder.transformer.resblocks.21.ln_1.bias', 'vision_encoder.transformer.resblocks.21.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.21.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.21.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.21.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_2.weight', 'vision_encoder.transformer.resblocks.21.ln_2.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_1.weight', 'vision_encoder.transformer.resblocks.22.ln_1.bias', 'vision_encoder.transformer.resblocks.22.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.22.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.22.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.22.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_2.weight', 'vision_encoder.transformer.resblocks.22.ln_2.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_1.weight', 'vision_encoder.transformer.resblocks.23.ln_1.bias', 'vision_encoder.transformer.resblocks.23.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.23.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.23.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.23.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_2.weight', 'vision_encoder.transformer.resblocks.23.ln_2.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.bias', 'vision_encoder.ln_post.weight', 'vision_encoder.ln_post.bias', 'lang_encoder.transformer.blocks.0.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.norm_f.weight', 'lang_encoder.old_decoder_blocks.0.norm_1.weight', 'lang_encoder.old_decoder_blocks.0.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.0.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.0.norm_2.weight', 'lang_encoder.old_decoder_blocks.0.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.0.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.1.norm_1.weight', 'lang_encoder.old_decoder_blocks.1.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.1.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.1.norm_2.weight', 'lang_encoder.old_decoder_blocks.1.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.1.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.2.norm_1.weight', 'lang_encoder.old_decoder_blocks.2.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.2.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.2.norm_2.weight', 'lang_encoder.old_decoder_blocks.2.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.2.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.3.norm_1.weight', 'lang_encoder.old_decoder_blocks.3.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.3.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.3.norm_2.weight', 'lang_encoder.old_decoder_blocks.3.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.3.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.4.norm_1.weight', 'lang_encoder.old_decoder_blocks.4.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.4.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.4.norm_2.weight', 'lang_encoder.old_decoder_blocks.4.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.4.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.5.norm_1.weight', 'lang_encoder.old_decoder_blocks.5.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.5.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.5.norm_2.weight', 'lang_encoder.old_decoder_blocks.5.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.5.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.6.norm_1.weight', 'lang_encoder.old_decoder_blocks.6.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.6.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.6.norm_2.weight', 'lang_encoder.old_decoder_blocks.6.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.6.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.7.norm_1.weight', 'lang_encoder.old_decoder_blocks.7.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.7.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.7.norm_2.weight', 'lang_encoder.old_decoder_blocks.7.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.7.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.8.norm_1.weight', 'lang_encoder.old_decoder_blocks.8.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.8.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.8.norm_2.weight', 'lang_encoder.old_decoder_blocks.8.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.8.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.9.norm_1.weight', 'lang_encoder.old_decoder_blocks.9.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.9.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.9.norm_2.weight', 'lang_encoder.old_decoder_blocks.9.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.9.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.10.norm_1.weight', 'lang_encoder.old_decoder_blocks.10.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.10.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.10.norm_2.weight', 'lang_encoder.old_decoder_blocks.10.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.10.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.11.norm_1.weight', 'lang_encoder.old_decoder_blocks.11.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.11.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.11.norm_2.weight', 'lang_encoder.old_decoder_blocks.11.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.11.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.12.norm_1.weight', 'lang_encoder.old_decoder_blocks.12.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.12.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.12.norm_2.weight', 'lang_encoder.old_decoder_blocks.12.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.12.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.13.norm_1.weight', 'lang_encoder.old_decoder_blocks.13.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.13.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.13.norm_2.weight', 'lang_encoder.old_decoder_blocks.13.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.13.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.14.norm_1.weight', 'lang_encoder.old_decoder_blocks.14.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.14.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.14.norm_2.weight', 'lang_encoder.old_decoder_blocks.14.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.14.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.15.norm_1.weight', 'lang_encoder.old_decoder_blocks.15.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.15.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.15.norm_2.weight', 'lang_encoder.old_decoder_blocks.15.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.15.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.16.norm_1.weight', 'lang_encoder.old_decoder_blocks.16.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.16.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.16.norm_2.weight', 'lang_encoder.old_decoder_blocks.16.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.16.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.17.norm_1.weight', 'lang_encoder.old_decoder_blocks.17.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.17.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.17.norm_2.weight', 'lang_encoder.old_decoder_blocks.17.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.17.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.18.norm_1.weight', 'lang_encoder.old_decoder_blocks.18.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.18.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.18.norm_2.weight', 'lang_encoder.old_decoder_blocks.18.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.18.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.19.norm_1.weight', 'lang_encoder.old_decoder_blocks.19.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.19.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.19.norm_2.weight', 'lang_encoder.old_decoder_blocks.19.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.19.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.20.norm_1.weight', 'lang_encoder.old_decoder_blocks.20.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.20.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.20.norm_2.weight', 'lang_encoder.old_decoder_blocks.20.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.20.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.21.norm_1.weight', 'lang_encoder.old_decoder_blocks.21.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.21.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.21.norm_2.weight', 'lang_encoder.old_decoder_blocks.21.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.21.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.22.norm_1.weight', 'lang_encoder.old_decoder_blocks.22.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.22.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.22.norm_2.weight', 'lang_encoder.old_decoder_blocks.22.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.22.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.23.norm_1.weight', 'lang_encoder.old_decoder_blocks.23.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.23.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.23.norm_2.weight', 'lang_encoder.old_decoder_blocks.23.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.23.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.24.norm_1.weight', 'lang_encoder.old_decoder_blocks.24.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.24.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.24.norm_2.weight', 'lang_encoder.old_decoder_blocks.24.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.24.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.25.norm_1.weight', 'lang_encoder.old_decoder_blocks.25.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.25.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.25.norm_2.weight', 'lang_encoder.old_decoder_blocks.25.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.25.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.26.norm_1.weight', 'lang_encoder.old_decoder_blocks.26.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.26.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.26.norm_2.weight', 'lang_encoder.old_decoder_blocks.26.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.26.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.27.norm_1.weight', 'lang_encoder.old_decoder_blocks.27.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.27.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.27.norm_2.weight', 'lang_encoder.old_decoder_blocks.27.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.27.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.28.norm_1.weight', 'lang_encoder.old_decoder_blocks.28.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.28.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.28.norm_2.weight', 'lang_encoder.old_decoder_blocks.28.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.28.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.29.norm_1.weight', 'lang_encoder.old_decoder_blocks.29.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.29.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.29.norm_2.weight', 'lang_encoder.old_decoder_blocks.29.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.29.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.30.norm_1.weight', 'lang_encoder.old_decoder_blocks.30.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.30.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.30.norm_2.weight', 'lang_encoder.old_decoder_blocks.30.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.30.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.31.norm_1.weight', 'lang_encoder.old_decoder_blocks.31.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.31.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.31.norm_2.weight', 'lang_encoder.old_decoder_blocks.31.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.31.ffn.down_proj.weight', 'lang_encoder.gated_cross_attn_layers.3.attn_gate', 'lang_encoder.gated_cross_attn_layers.3.ff_gate', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.3.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.3.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.7.attn_gate', 'lang_encoder.gated_cross_attn_layers.7.ff_gate', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.7.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.7.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.11.attn_gate', 'lang_encoder.gated_cross_attn_layers.11.ff_gate', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.11.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.11.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.15.attn_gate', 'lang_encoder.gated_cross_attn_layers.15.ff_gate', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.15.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.15.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.19.attn_gate', 'lang_encoder.gated_cross_attn_layers.19.ff_gate', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.19.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.19.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.23.attn_gate', 'lang_encoder.gated_cross_attn_layers.23.ff_gate', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.23.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.23.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.27.attn_gate', 'lang_encoder.gated_cross_attn_layers.27.ff_gate', 'lang_encoder.gated_cross_attn_layers.27.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.27.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.27.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.31.attn_gate', 'lang_encoder.gated_cross_attn_layers.31.ff_gate', 'lang_encoder.gated_cross_attn_layers.31.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.31.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.31.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.3.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flamingo(\n",
       "  (vision_encoder): VisionTransformer(\n",
       "    (patchnorm_pre_ln): Identity()\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (perceiver): PerceiverResampler(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x ModuleList(\n",
       "        (0): PerceiverAttention(\n",
       "          (norm_media): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm_latents): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=1024, bias=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lang_encoder): MPTForCausalLM(\n",
       "    (transformer): MPTModel(\n",
       "      (wte): Embedding(50280, 4096)\n",
       "      (emb_drop): Dropout(p=0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4-6): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8-10): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12-14): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16-18): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20-22): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (24-26): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (27): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (28-30): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (31): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (old_decoder_blocks): ModuleList(\n",
       "      (0-31): 32 x MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (gated_cross_attn_layers): ModuleList(\n",
       "      (0-2): 3 x None\n",
       "      (3): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4-6): 3 x None\n",
       "      (7): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (8-10): 3 x None\n",
       "      (11): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12-14): 3 x None\n",
       "      (15): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-18): 3 x None\n",
       "      (19): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (20-22): 3 x None\n",
       "      (23): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (24-26): 3 x None\n",
       "      (27): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (28-30): 3 x None\n",
       "      (31): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>main question is What is on the wall?. Write a intermediate question to clarify entities of the main question. Intermediate question: Which part of the wall? <|endofchunk|><image>main question is Is the woman to the left or to the right of the man?. Write a intermediate question to clarify entities of the main question. Intermediate question: What is the man wearing? <|endofchunk|><image>main question is Are the bananas?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the bananas is in the bottom of the image?  <|endofchunk|><image>main question is On which side of the image is the cup?. Write a intermediate question to clarify entities of the main question. Intermediate question: What is the color of the cup? <|endofchunk|><image>main question is Is the man to the right of the surfboard?. Write a intermediate question to clarify entities of the main question. Intermediate question: where is the surfboard in the picture? <|endofchunk|><image>main question is What is under the sign?. Write a intermediate question to clarify entities of the main question. Intermediate question: Does the sign look red and yellow?    <|endofchunk|><image>main question is Is the large window above the sign?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the sign on the sidewalk? <|endofchunk|><image>main question is Is the bag to the right or to the left of the woman?. Write a intermediate question to clarify entities of the main question. Intermediate question: what is the woman wearing? <|endofchunk|><image>main question is Is the helmet to the right or to the left of the man?. Write a intermediate question to clarify entities of the main question. Intermediate question: What is the color of the helmet? <|endofchunk|><image>main question is Who is balancing on the skateboard?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the skateboard in the middle of the image? <|endofchunk|><image>main question is Is the tent to the right or to the left of the person?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the person is in the left of the image? <|endofchunk|><image>main question is Is the chair?. Write a intermediate question to clarify entities of the main question. Intermediate question: What is the color of the pillow? <|endofchunk|><image>main question is Does the kite have triangular shape and large size?. Write a intermediate question to clarify entities of the main question. Intermediate question: Does the kite have a red color? <|endofchunk|><image>main question is Who is looking at the elephant?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the elephant on the left side of the picture? <|endofchunk|><image>main question is What are the vegetables in the bowl?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the bowl located southeast of the image? <|endofchunk|><image>main question is Is the woman wearing a skirt?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the woman holding a cup? <|endofchunk|><image>main question is What color is the bottle?. Write a intermediate question to clarify entities of the main question. Intermediate question: Which side of the picture is the bottle on? <|endofchunk|><image>main question is Is the man wearing a hat?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the man wearing black pants? <|endofchunk|><image>main question is What color is the bag?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the woman carrying the bag? <|endofchunk|><image>main question is Does the umbrella look white and open?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the umbrella on the woman's right? <|endofchunk|><image>main question is What is the cow doing?. Write a intermediate question to clarify entities of the main question. Intermediate question: What color does the cow have? <|endofchunk|><image>main question is Is the red vehicle to the right or to the left of the car?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the car white? <|endofchunk|><image>main question is Does the skateboard look colorful?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the skateboard under the man? <|endofchunk|><image>main question is What color do you think the jeans are?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the jumping person  wearing the jeans? <|endofchunk|><image>main question is What is on the chair?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the chair to the right of the pot? <|endofchunk|><image>main question is Is the umbrella covering a nightstand?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the umbrella next to the window? <|endofchunk|><image>main question is Is the player wearing shorts?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the player wearing black gloves? <|endofchunk|><image>main question is Who is on the motorcycle?. Write a intermediate question to clarify entities of the main question. Intermediate question: What is the color of the motorcycle? <|endofchunk|><image>main question is What is the woman wearing?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the woman is to right of the bucket? <|endofchunk|><image>main question is Is the woman to the right or to the left of the man?. Write a intermediate question to clarify entities of the main question. Intermediate question: What is the man doing? <|endofchunk|><image>main question is Is the man to the right or to the left of the chair?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the chair in front of the table? <|endofchunk|><image>main question is Is the dishwasher to the right or to the left of the chair?. Write a intermediate question to clarify entities of the main question. Intermediate question: Is the chair is in middle of the picture? <|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "num_few_shot_examples = len(few_shot_examples)\n",
    "\n",
    "few_shot_image_ids = [example[\"imageId\"] for example in few_shot_examples.values()]\n",
    "few_shot_images = [Image.open(\"./images/\" + str(image_id) + \".jpg\").convert('RGB') for image_id in few_shot_image_ids]\n",
    "\n",
    "vision_x = [image_processor(demo_image).unsqueeze(0) for demo_image in few_shot_images]\n",
    "                \n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "\n",
    "text = [f\"<image>Main question is {few_shot_example['ambiguous_question']}. \" +   \n",
    "        \"Write a intermediate question to clarify entities of the main question. \" + \n",
    "        f\"Intermediate question: {few_shot_example['intermediate_question']} <|endofchunk|>\"  for qid, few_shot_example in few_shot_examples.items()]\n",
    "\n",
    "        # \"The entitites of question: \" +  \" \".join(few_shot_example['question_entities']) + \". \" + \n",
    "        \n",
    "\n",
    "text = \"\".join(text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "2it [01:16, 38.23s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Is the bus to the left or to the right of the pedestrians?\n",
      "Generated text:  What is the color of the bus? main question is Is the man to the left or to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:35, 55.18s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the food to the left of the doughnut the bucket is to the right of?\n",
      "Generated text:  What is the color of the bucket? main question is What is the color of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [03:54, 63.95s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  How large is the bear?\n",
      "Generated text:  Is the bear in the middle of the image? main question is What is the color of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:12, 69.13s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  Is the bus to the right or to the left of the driver?\n",
      "Generated text:  What is the color of the bus? main question is Is the man to the right or to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [06:31, 72.40s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "7it [07:50, 74.33s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "8it [09:08, 75.67s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "9it [10:27, 76.70s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "10it [11:46, 77.32s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "11it [13:05, 77.83s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "12it [14:24, 78.26s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "13it [15:43, 78.45s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "14it [17:02, 78.61s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "15it [18:21, 78.58s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "16it [19:39, 78.55s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "17it [20:58, 78.67s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "18it [22:17, 78.68s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "19it [23:35, 78.66s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "20it [24:54, 78.75s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "21it [26:13, 78.83s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "22it [27:32, 78.82s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "23it [28:51, 78.90s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "24it [30:10, 78.85s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "25it [31:29, 78.74s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "26it [32:47, 78.70s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "27it [34:06, 78.66s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "28it [35:24, 78.69s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "29it [36:43, 78.78s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "30it [38:03, 78.86s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "31it [39:22, 78.93s/it]Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "32it [40:40, 76.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the color of the bus? main question is Is the man to the left or to', 'What is the color of the bucket? \\xa0main question is What is the color of the', 'Is the bear in the middle of the image? main question is What is the color of the', 'What is the color of the bus? main question is Is the man to the right or to', 'What is the color of the car? main question is Is the man to the right or to', 'Is the elephant is in the middle of the image? main question is Is the man to the', 'Is the picture in the middle of the image? main question is Which side of the image is', 'What is the color of the phone? main question is Is the man to the right or to', 'What is the color of the plate? main question is What is the man doing?. Write a', 'What is the color of the television? main question is Is the person to the right or to', 'Is the chair to the left of the remote control that is to the left of the girl?', 'What is the man wearing? main question is Is the man to the right or to the left', 'Is the cow in the middle of the picture? main question is Is the man to the right', 'Is the zebra is in the middle of the image? main question is Is the man to the', 'Is the elephant is in the middle of the image? main question is Is the man to the', 'Is the cucumber in the middle of the image? \\xa0main question is What is the', 'What is the color of the picture? main question is Is the man to the left or to', 'Is the table in the middle of the image? main question is What is the color of the', 'What is the item of furniture to the right of the speaker? main question is What is the', 'What is the color of the glasses? main question is Is the man to the right or to', 'What is the color of the car? main question is Is the man to the right or to', 'What is the woman wearing? main question is Is the man to the right or to the left', 'What is the color of the building? main question is Is the man to the right or to', 'Is the people is in the middle of the image? main question is Is the man to the', 'What is the color of the shirt? main question is Is the man to the right or to', 'What is the color of the shirt? main question is What is the man doing?. Write a', 'What is the color of the backpack? main question is Is the person to the right or to', 'Is the cell phone in the middle of the image? main question is Is the man to the', 'What is the color of the shirt? main question is Is the man to the right or to', 'What is the item of furniture that is to the left of the curtains? main question is Is', 'What is the color of the car? main question is Is the man to the right or to']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for idx, (qid, example)in tqdm(enumerate(test_examples.items())):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    question = example[\"ambiguous_question\"]\n",
    "    image_id = example['imageId']\n",
    "    image = Image.open(\"./images/\" + str(image_id) + '.jpg').convert('RGB')\n",
    "    input_prompt = text + \\\n",
    "    f\"<image>Main question is {example['ambiguous_question']}. \" +  \\\n",
    "    \"Write a intermediate question to clarify entities of the main question. \" + \\\n",
    "    f\"Intermediate question: \"\n",
    "    \n",
    "    \n",
    "    # \"The entitites of question : \" +  \" \".join(example['question_entities']) + \". \" + \\\n",
    "    \n",
    "    # print(input_prompt)\n",
    "    \n",
    "    input_images = vision_x + [image]\n",
    "    input_images = torch.cat(vision_x, dim=0)\n",
    "    input_images = input_images.unsqueeze(1).unsqueeze(0)\n",
    "    \n",
    "    lang_x = tokenizer(\n",
    "        [input_prompt] ,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    generated_text = model.generate(\n",
    "    vision_x=input_images.to(device),\n",
    "    lang_x=lang_x[\"input_ids\"].to(device),\n",
    "    attention_mask=lang_x[\"attention_mask\"].to(device),\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    "    )\n",
    "    \n",
    "    #print(tokenizer.batch_decode(lang_x[\"input_ids\"],skip_special_tokens=False))\n",
    "    #print(\"Input prompt: \", input_prompt)\n",
    "    \n",
    "    decoded_text =  tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "    if idx < 5:\n",
    "        print(\"question: \", question)\n",
    "        print(\"Generated text: \",decoded_text.split(\"Intermediate question:\")[-1].strip())\n",
    "        # print()\n",
    "    intermediate_question = decoded_text.split(\"Intermediate question:\")[-1].strip()\n",
    "\n",
    "    #inputs = processor(image, input_promt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    #out = model.generate(**inputs)\n",
    "    #prediction = processor.decode(out[0], skip_special_tokens=True)\n",
    "    #print(prediction)\n",
    "    predictions.append(intermediate_question)\n",
    "    \n",
    "\n",
    "print(predictions)\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:00, 74153.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the man 's hair?\n",
      "Is the bus to the left or to the right of the pedestrians?\n",
      "What is the food to the left of the doughnut the bucket is to the right of?\n",
      "How large is the bear?\n",
      "Is the bus to the right or to the left of the driver?\n",
      "Is the blue car to the right or to the left of the van?\n",
      "Is the elephant?\n",
      "Which side is the picture on?\n",
      "What device is the girl holding , a phone or a Wii controller?\n",
      "What is the food on the plate called?\n",
      "Is the person to the left of the laptop looking at a television?\n",
      "Is the chair to the left of the remote control that is to the left of the girl?\n",
      "What is the person to the left of the man holding?\n",
      "Is the house to the right or to the left of the cow?\n",
      "What color is the zebra?\n",
      "What is the elephant?\n",
      "What is the vegetable to the left of the cucumber called?\n",
      "Is she to the left of a picture?\n",
      "What shape does the table have?\n",
      "What is the item of furniture to the left of the speaker?\n",
      "Are the glasses to the right or to the left of the person?\n",
      "Which side of the picture is the car on?\n",
      "What is the appliance to the right of the woman sitting on top of?\n",
      "What is on the building behind the window?\n",
      "Are the people?\n",
      "What color is the shirt?\n",
      "What is the player wearing?\n",
      "Is the person in the snow wearing a backpack?\n",
      "What type of device is open , the cell phone or the microphone?\n",
      "Is the man to the right of the people sitting on a bench?\n",
      "What is the item of furniture that is to the right of the curtains?\n",
      "Is the car?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (qid, example)in tqdm(enumerate(test_examples.items())):\n",
    "    print(example['ambiguous_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
